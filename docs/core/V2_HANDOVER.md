# ğŸ“‹ ProjectLodestar v2 Planning Handover Document

**Date:** February 8, 2026  
**Current Version:** v1.0.0 (Production Ready)  
**Next Version:** v2.0.0 (Planning Phase)  
**Project:** https://github.com/zebadee2kk/ProjectLodestar

---

## ğŸ¯ Executive Summary

ProjectLodestar is a **production-ready AI development environment** that achieves **90% cost savings** by intelligently routing between 8 LLM providers while defaulting to FREE local models (DeepSeek Coder, Llama 3.1) running on a GPU VM.

**Status:** v1.0.0 shipped, fully operational, battle-tested.

**Key Achievement:** Users can code with AI assistance for free indefinitely, with the option to escalate to premium models (Claude, OpenAI, Grok, Gemini) only when needed.

---

## ğŸ—ï¸ Current Architecture (v1.0.0)

### System Components
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Aider (CLI)    â”‚  â† User interface for AI pair programming
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ OpenAI-compatible API
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LiteLLM Router â”‚  â† Smart routing layer (localhost:4000)
â”‚  (Debian VM)    â”‚  â† Routes requests based on model alias
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                               â”‚
    â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FREE Models  â”‚           â”‚ PAID APIs    â”‚
â”‚              â”‚           â”‚              â”‚
â”‚ T600 GPU VM  â”‚           â”‚ Cloud APIs   â”‚
â”‚ 192.168.120. â”‚           â”‚ (Internet)   â”‚
â”‚ 211:11434    â”‚           â”‚              â”‚
â”‚              â”‚           â”‚              â”‚
â”‚ â€¢ DeepSeek   â”‚           â”‚ â€¢ Claude     â”‚
â”‚   Coder 6.7B â”‚           â”‚ â€¢ OpenAI     â”‚
â”‚ â€¢ Llama 3.1  â”‚           â”‚ â€¢ Grok       â”‚
â”‚   8B         â”‚           â”‚ â€¢ Gemini     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Infrastructure Details

**Main VM (vm-lodestar-core):**
- OS: Debian 12
- IP: 192.168.120.40
- Role: Router host, development environment
- Tools: Aider, LiteLLM, Git, Python 3.11

**T600 GPU VM:**
- IP: 192.168.120.211
- Port: 11434
- Role: Ollama server for local models
- GPU: NVIDIA T600 (4GB VRAM)
- Models: DeepSeek Coder 6.7B (Q4), Llama 3.1 8B (Q4)

**Network:**
- DNS: 192.168.120.10, 1.1.1.1, 8.8.8.8
- Router: localhost:4000 (LiteLLM)
- Ollama: 192.168.120.211:11434

---

## ğŸ“ Repository Structure
```
ProjectLodestar/
â”œâ”€â”€ README.md                    # Main documentation
â”œâ”€â”€ ROADMAP.md                   # v2 planning
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ .gitignore                   # Protects API keys, logs, Python artifacts
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ litellm_config.yaml      # 8 provider configurations
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start-router.sh          # Start LiteLLM with readiness check
â”‚   â”œâ”€â”€ stop-router.sh           # Stop router gracefully
â”‚   â”œâ”€â”€ status.sh                # Health check all components
â”‚   â”œâ”€â”€ test-lodestar.sh         # Infrastructure test (E2E)
â”‚   â”œâ”€â”€ test-providers-simple.sh # Quick provider test
â”‚   â”œâ”€â”€ test-all-providers.sh    # Comprehensive provider test
â”‚   â”œâ”€â”€ quick-start.sh           # Daily startup routine
â”‚   â””â”€â”€ adr-new.sh               # Create new ADR
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md          # System design
â”‚   â”œâ”€â”€ SETUP.md                 # Installation guide
â”‚   â”œâ”€â”€ WORKFLOW.md              # Usage patterns
â”‚   â”œâ”€â”€ SECURITY.md              # API key management
â”‚   â”œâ”€â”€ CONTRIBUTING.md          # Contribution guidelines
â”‚   â”œâ”€â”€ QuickRef.md              # Quick reference guide
â”‚   â””â”€â”€ adr/                     # 3 Architecture Decision Records
â”‚
â”œâ”€â”€ .claude/                     # AI development context
â”‚   â”œâ”€â”€ PROJECT_INSTRUCTIONS.md  # v2 development guidelines
â”‚   â”œâ”€â”€ ARCHITECTURE_PRINCIPLES.md # Design philosophy
â”‚   â”œâ”€â”€ MODULE_TEMPLATE.md       # Module scaffolding template
â”‚   â””â”€â”€ V2_HANDOVER.md           # v2 planning handover
â”‚
â””â”€â”€ .lodestar/                   # Runtime (not in git)
    â”œâ”€â”€ router.log               # LiteLLM logs
    â””â”€â”€ router.pid               # Process ID
```

---

## ğŸ”§ Current Configuration

### Provider Configuration

**Tier 1 - FREE (Default):**
- `gpt-3.5-turbo` â†’ `ollama/deepseek-coder:6.7b` @ T600
- `local-llama` â†’ `ollama/llama3.1:8b` @ T600

**Tier 2 - Claude:**
- `claude-sonnet` â†’ `anthropic/claude-sonnet-4-5-20250929`
- `claude-opus` â†’ `anthropic/claude-opus-4-5-20251101`

**Tier 3 - OpenAI:**
- `gpt-4o-mini` â†’ `openai/gpt-4o-mini`
- `gpt-4o` â†’ `openai/gpt-4o`

**Tier 4 - Grok:**
- `grok-beta` â†’ `xai/grok-beta`

**Tier 5 - Gemini:**
- `gemini-pro` â†’ `gemini-1.5-flash`

---

## âœ… What's Working (v1.0.0)

- âœ… **DeepSeek Coder** - FREE, unlimited, fast responses
- âœ… **Llama 3.1** - FREE, unlimited, good for general coding
- âœ… **Router** - Auto-starts, graceful fallbacks
- âœ… **Git Integration** - Auto-commits preserve context
- âœ… **SSH Auth** - No password pushes to GitHub
- âœ… **Testing Suite** - Automated validation
- âœ… **Documentation** - 12 docs + 7 ADRs

### Verified Routing (Needs Credits)

- ğŸ’³ **Claude Sonnet/Opus** - Routing works, needs API credits
- ğŸ’³ **OpenAI GPT-4o/Mini** - Routing works, needs API credits
- ğŸ’³ **Grok Beta** - Routing works, needs API credits

---

## ğŸ¯ v2.0 USER-REQUESTED FEATURES

### 1. **Learning Module** ğŸ§  HIGH PRIORITY

**User Description:**
> "A module that 'learns' from the responses received from the public LLM and retrains the information back into the local LLMs"

**Concept:**
- Collect responses from Claude/GPT-4o when used
- Extract high-quality code patterns/solutions
- Create fine-tuning dataset
- Periodically retrain DeepSeek/Llama
- Measure improvement over time

**Potential Approach:**
```
User Request â†’ Claude (expensive)
              â†“
         [Response Logger]
              â†“
    [Quality Filter]
              â†“
    [Dataset Builder - JSONL]
              â†“
    [Fine-tuning Pipeline - weekly]
              â†“
    Updated Models
```

**Challenges:**
- Training requires significant GPU resources
- Fine-tuning 6.7B model takes hours
- Need quality control
- Avoid overfitting

**ADR Needed:** 
- Architecture for response collection
- Training pipeline design
- Quality metrics

---

### 2. **Usage Tracking & Session Limits** ğŸ“Š HIGH PRIORITY

**User Description:**
> "A module to track usage including session limits etc"

**Requirements:**
- Track tokens used per model
- Track cost per model
- Session time limits
- Daily/weekly/monthly budgets
- Alerts when approaching limits
- Per-project tracking
- Export reports

**Potential Features:**
```
Usage Dashboard:
â”œâ”€â”€ Real-time token counter
â”œâ”€â”€ Cost accumulator
â”œâ”€â”€ Session timer
â”œâ”€â”€ Budget alerts
â”œâ”€â”€ Model usage breakdown
â”œâ”€â”€ Historical trends
â””â”€â”€ Export to CSV/JSON
```

**Storage Options:**
- SQLite database (`~/.lodestar/usage.db`)
- JSON logs for backups
- 90-day retention default

**ADR Needed:**
- Storage architecture
- Alert mechanisms
- Privacy considerations

---

## ğŸ› ï¸ Development Approach

### Modular Architecture (CRITICAL)

Every feature MUST be:
- Self-contained module
- Can be enabled/disabled via config
- Tested in isolation
- Removed without breaking system

**Structure:**
```
modules/
â”œâ”€â”€ usage_tracker/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tracker.py
â”‚   â”œâ”€â”€ storage.py
â”‚   â”œâ”€â”€ reporter.py
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ learning/
â”‚   â””â”€â”€ [same structure]
â””â”€â”€ health_monitor/
    â””â”€â”€ [same structure]
```

### Configuration-Driven
```yaml
# config/modules.yaml
modules:
  usage_tracker:
    enabled: true
    database: ~/.lodestar/usage.db
    
  learning:
    enabled: false  # Can disable easily
```

---

## ğŸ“‹ Recommended Development Sequence

### Phase 1: Usage Tracking MVP (Week 1-2)

**Sprint 1.1:**
1. Create SQLite schema
2. Hook into LiteLLM logging
3. Basic CLI report
4. Budget alerts

**Deliverable:** Can see token usage and costs

---

**Sprint 1.2:**
1. Response collection
2. Quality filter
3. Dataset builder

**Deliverable:** Collecting quality responses

---

### Phase 2: Learning Pipeline (Week 3-4)

**Sprint 2.1:**
1. Test LoRA fine-tuning
2. Validate improvements
3. Document process
4. Automated training script

**Deliverable:** Proof of concept

---

**Sprint 2.2:**
1. Weekly auto-training
2. Model versioning
3. A/B testing
4. Rollback mechanism

**Deliverable:** Continuous learning operational

---

## ğŸ” Open Questions

### Learning Module

1. Training frequency? Daily/Weekly/Monthly?
2. Quality metrics?
3. Data mix ratio?
4. Fine-tune DeepSeek or Llama or both?
5. Where to run training? T600 too small (4GB VRAM)

### Usage Tracking

1. What data to store? Just tokens or prompts?
2. Retention period? 30 days? Forever?
3. Alert method? Email/CLI?
4. Multi-user support?
5. Actual billing integration?

---

## ğŸ¯ Immediate Next Steps

1. **Read this handover**
2. **Review current codebase**
3. **Test current system**
4. **Choose first feature** (Usage Tracking recommended)
5. **Create ADR**
6. **Prototype MVP**
7. **Test & Iterate**
8. **Document & Commit**

### Recommended Starting Point

**OPTION A - Quick Win:**
Start with **Usage Tracking** (simpler, immediate value)
- Ship v2.1 in 1-2 weeks

**OPTION B - Ambitious:**
Start with **Learning Module** (complex, high impact)
- Ship v2.0 in 4-6 weeks

**Recommendation:** Start with Usage Tracking (quick win builds momentum)

---

## ğŸ’¾ Key Files & Paths

**Configuration:**
- LiteLLM: `/home/lodestar/ProjectLodestar/config/litellm_config.yaml`
- Aider: `/home/lodestar/.aider.conf.yml`
- API keys: `/home/lodestar/.bashrc`

**Scripts:**
- Router: `scripts/start-router.sh`
- Tests: `scripts/test-*.sh`
- ADR: `scripts/adr-new.sh`

**Documentation:**
- Main: `README.md`
- Architecture: `docs/ARCHITECTURE.md`
- ADRs: `docs/adr/`

---

## âœ… System State

**Status:** v1.0.0 fully operational and stable

- âœ… v1.0.0 tagged and released
- âœ… All documentation updated
- âœ… GitHub project configured
- âœ… SSH authentication working
- âœ… Test suite passing
- âœ… System operational

**Ready for v2 development to begin!** ğŸš€

---

**Contact:** Rich (IT Director, UK)  
**VM Access:** `ssh lodestar@192.168.120.40`  
**GitHub:** https://github.com/zebadee2kk/ProjectLodestar
