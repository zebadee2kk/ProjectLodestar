INFO:     Started server process [4104]
INFO:     Waiting for application startup.
[92m14:41:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:554 - litellm.proxy.proxy_server.py::startup() - CHECKING PREMIUM USER - False
[92m14:41:35 - LiteLLM Proxy:DEBUG[0m: litellm_license.py:102 - litellm.proxy.auth.litellm_license.py::is_premium() - ENTERING 'IS_PREMIUM' - LiteLLM License=None
[92m14:41:35 - LiteLLM Proxy:DEBUG[0m: litellm_license.py:111 - litellm.proxy.auth.litellm_license.py::is_premium() - Updated 'self.license_str' - None
[92m14:41:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:567 - worker_config: {"model": null, "alias": null, "api_base": null, "api_version": "2024-07-01-preview", "debug": false, "detailed_debug": true, "temperature": null, "max_tokens": null, "request_timeout": null, "max_budget": null, "telemetry": true, "drop_params": false, "add_function_to_prompt": false, "headers": null, "save": false, "config": "config/litellm_config.yaml", "use_queue": false}

   ‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó
   ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë
   ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë
   ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë
   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë
   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù

[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'combined_model_name': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'stripped_model_name': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'combined_stripped_model_name': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'custom_llm_provider': None}
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:4837 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:2313 - added/updated model=8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b in litellm.model_cost: 8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'ollama/deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:4837 - Error getting model info: OllamaError: Error getting model info for deepseek-coder:6.7b. Set Ollama API Base via `OLLAMA_API_BASE` environment variable. Error: [Errno 111] Connection refused
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:2313 - added/updated model=ollama/deepseek-coder:6.7b in litellm.model_cost: ollama/deepseek-coder:6.7b
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'ad394dbf9e3e7c2fd5dec38f30d9540d08e6b76cb8e9c6a2310a71f2fea2c11a', 'combined_model_name': 'ad394dbf9e3e7c2fd5dec38f30d9540d08e6b76cb8e9c6a2310a71f2fea2c11a', 'stripped_model_name': 'ad394dbf9e3e7c2fd5dec38f30d9540d08e6b76cb8e9c6a2310a71f2fea2c11a', 'combined_stripped_model_name': 'ad394dbf9e3e7c2fd5dec38f30d9540d08e6b76cb8e9c6a2310a71f2fea2c11a', 'custom_llm_provider': None}
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:4837 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:2313 - added/updated model=ad394dbf9e3e7c2fd5dec38f30d9540d08e6b76cb8e9c6a2310a71f2fea2c11a in litellm.model_cost: ad394dbf9e3e7c2fd5dec38f30d9540d08e6b76cb8e9c6a2310a71f2fea2c11a
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama3.1:8b', 'combined_model_name': 'ollama/llama3.1:8b', 'stripped_model_name': 'ollama/llama3.1:8b', 'combined_stripped_model_name': 'ollama/llama3.1:8b', 'custom_llm_provider': 'ollama'}
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:4837 - Error getting model info: OllamaError: Error getting model info for llama3.1:8b. Set Ollama API Base via `OLLAMA_API_BASE` environment variable. Error: [Errno 111] Connection refused
[92m14:41:35 - LiteLLM:DEBUG[0m: utils.py:2313 - added/updated model=ollama/llama3.1:8b in litellm.model_cost: ollama/llama3.1:8b
[92m14:41:35 - LiteLLM Router:DEBUG[0m: router.py:5054 - 
Initialized Model List ['gpt-3.5-turbo', 'local-llama']
[92m14:41:35 - LiteLLM Router:INFO[0m: router.py:696 - Routing strategy: simple-shuffle
[92m14:41:35 - LiteLLM Router:DEBUG[0m: router.py:562 - Intialized router with Routing strategy: simple-shuffle

Routing enable_pre_call_checks: False

Routing fallbacks: None

Routing content fallbacks: None

Routing context window fallbacks: None

Router Redis Caching=None

[92m14:41:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:641 - prisma_client: None
[92m14:41:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)
[92m14:41:36 - LiteLLM Proxy:DEBUG[0m: model_checks.py:121 - ALL KEY MODELS - 0
[92m14:41:36 - LiteLLM Proxy:DEBUG[0m: model_checks.py:153 - ALL TEAM MODELS - 0

[1;37m#------------------------------------------------------------#[0m
[1;37m#                                                            #[0m
[1;37m#           'I get frustrated when the product...'            #[0m
[1;37m#        https://github.com/BerriAI/litellm/issues/new        #[0m
[1;37m#                                                            #[0m
[1;37m#------------------------------------------------------------#[0m

 Thank you for using LiteLLM! - Krrish & Ishaan



[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m


[32mLiteLLM: Proxy initialized with Config, Set models:[0m
[32m    gpt-3.5-turbo[0m
[32m    local-llama[0m
INFO:     127.0.0.1:34482 - "GET /v1/models HTTP/1.1" 200 OK
[92m14:41:36 - LiteLLM Proxy:DEBUG[0m: model_checks.py:121 - ALL KEY MODELS - 0
[92m14:41:36 - LiteLLM Proxy:DEBUG[0m: model_checks.py:153 - ALL TEAM MODELS - 0
INFO:     127.0.0.1:34492 - "GET /v1/models HTTP/1.1" 200 OK
[92m14:41:36 - LiteLLM Proxy:DEBUG[0m: model_checks.py:121 - ALL KEY MODELS - 0
[92m14:41:36 - LiteLLM Proxy:DEBUG[0m: model_checks.py:153 - ALL TEAM MODELS - 0
INFO:     127.0.0.1:34500 - "GET /v1/models HTTP/1.1" 200 OK
[92m14:43:10 - LiteLLM Proxy:DEBUG[0m: model_checks.py:121 - ALL KEY MODELS - 0
[92m14:43:10 - LiteLLM Proxy:DEBUG[0m: model_checks.py:153 - ALL TEAM MODELS - 0
INFO:     127.0.0.1:54942 - "GET /v1/models HTTP/1.1" 200 OK
[92m14:43:10 - LiteLLM Proxy:DEBUG[0m: model_checks.py:121 - ALL KEY MODELS - 0
[92m14:43:10 - LiteLLM Proxy:DEBUG[0m: model_checks.py:153 - ALL TEAM MODELS - 0
INFO:     127.0.0.1:54950 - "GET /v1/models HTTP/1.1" 200 OK
[92m14:43:19 - LiteLLM Proxy:DEBUG[0m: common_request_processing.py:388 - Request received by LiteLLM:
{
    "messages": [
        {
            "role": "system",
            "content": "Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven't been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user's project.\n\nUse the appropriate shell based on the user's system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        },
        {
            "role": "user",
            "content": "Change get_factorial() to use math.factorial"
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "Refactor hello() into its own file."
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "I switched to a new code base. Please don't consider the above files or try to edit them any longer."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "I am not sharing any files that you can edit yet."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "Create a file called hello.py with a function called greet() that prints 'Hello from Lodestar'"
        },
        {
            "role": "system",
            "content": "# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        }
    ],
    "model": "gpt-3.5-turbo",
    "stream": true,
    "temperature": 0
}
[92m14:43:19 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:664 - Request Headers: Headers({'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10600'})
[92m14:43:19 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:670 - receiving data: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create a file called hello.py with a function called greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10600'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create a file called hello.py with a function called greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10600'}}}
[92m14:43:19 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:839 - [PROXY] returned data from litellm_pre_call_utils: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create a file called hello.py with a function called greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10600'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create a file called hello.py with a function called greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10600'}}, 'metadata': {'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10600'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': ''}}
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:349 - Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510>, <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0>, <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0>]
[92m14:43:19 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {}
[92m14:43:19 - LiteLLM Proxy:DEBUG[0m: utils.py:1025 - Inside Proxy Logging Pre-call hook!
[92m14:43:19 - LiteLLM Proxy:DEBUG[0m: max_budget_limiter.py:23 - Inside Max Budget Limiter Pre-Call Hook
[92m14:43:19 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - Inside Max Parallel Request Pre-Call Hook
[92m14:43:19 - LiteLLM Proxy:INFO[0m: parallel_request_limiter.py:68 - Current Usage of key in this minute: None
[92m14:43:19 - LiteLLM Proxy:DEBUG[0m: cache_control_check.py:27 - Inside Cache Control Check Pre-Call Hook
[92m14:43:19 - LiteLLM Router:DEBUG[0m: router.py:3959 - Inside async function with retries.
[92m14:43:19 - LiteLLM Router:DEBUG[0m: router.py:3979 - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x7f4a04698690>>, num_retries - 3
[92m14:43:19 - LiteLLM Router:DEBUG[0m: router.py:6621 - initial list of deployments: [{'model_name': 'gpt-3.5-turbo', 'litellm_params': {'api_base': 'http://192.168.120.211:11434', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'ollama/deepseek-coder:6.7b'}, 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}}]
[92m14:43:19 - LiteLLM Router:DEBUG[0m: cooldown_handlers.py:342 - retrieve cooldown models: []
[92m14:43:19 - LiteLLM Router:DEBUG[0m: router.py:6688 - async cooldown deployments: []
[92m14:43:19 - LiteLLM Router:DEBUG[0m: router.py:6691 - cooldown_deployments: []
[92m14:43:19 - LiteLLM Router:DEBUG[0m: router.py:7055 - cooldown deployments: []
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:349 - [92mRequest to litellm:[0m
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:349 - [92mlitellm.acompletion(api_base='http://192.168.120.211:11434', use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='ollama/deepseek-coder:6.7b', messages=[{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create a file called hello.py with a function called greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], caching=False, client=None, temperature=0, proxy_server_request={'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10600'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create a file called hello.py with a function called greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, secret_fields={'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10600'}}, metadata={'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10600'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': '', 'model_group': 'gpt-3.5-turbo', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'ollama/deepseek-coder:6.7b', 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, 'api_base': 'http://192.168.120.211:11434', 'deployment_model_name': 'gpt-3.5-turbo', 'caching_groups': None}, litellm_call_id='6bd15a78-6518-4be8-9a1b-08d928456a5d', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x7f4a040edc50>, stream=True, litellm_trace_id='25ac1a17-27ab-43a9-977b-7d274bc8bb03', model_info={'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, timeout=600, max_retries=0)[0m
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:349 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
[92m14:43:19 - LiteLLM:DEBUG[0m: caching_handler.py:213 - CACHE RESULT: None
[92m14:43:19 - LiteLLM:INFO[0m: utils.py:3260 - 
LiteLLM completion() model= deepseek-coder:6.7b; provider = ollama
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:3263 - 
LiteLLM: Params passed to completion() {'model': 'deepseek-coder:6.7b', 'functions': None, 'function_call': None, 'temperature': 0, 'top_p': None, 'n': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create a file called hello.py with a function called greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'thinking': None, 'web_search_options': None}
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:3266 - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:349 - Final returned optional params: {'temperature': 0, 'stream': True}
[92m14:43:19 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:43:19 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:43:19 - LiteLLM:DEBUG[0m: litellm_logging.py:930 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
http://192.168.120.211:11434/api/generate \
-d '{'model': 'deepseek-coder:6.7b', 'prompt': '### System:\nAct as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n### User:\nChange get_factorial() to use math.factorial\n\n### Assistant:\nTo make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n\n### User:\nRefactor hello() into its own file.\n\n### Assistant:\nTo make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n\n### User:\nI switched to a new code base. Please don\'t consider the above files or try to edit them any longer.\n\n### Assistant:\nOk.\n\n### User:\nI am not sharing any files that you can edit yet.\n\n### Assistant:\nOk.\n\n### User:\nCreate a file called hello.py with a function called greet() that prints \'Hello from Lodestar\'\n\n### System:\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n', 'options': {'temperature': 0}, 'stream': True, 'images': []}'
[0m

[92m14:43:19 - LiteLLM:DEBUG[0m: http_handler.py:579 - Using AiohttpTransport...
[92m14:43:19 - LiteLLM:DEBUG[0m: http_handler.py:636 - Creating AiohttpTransport...
[92m14:44:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m14:44:34 - LiteLLM:DEBUG[0m: utils.py:349 - RAW RESPONSE:
first stream response received


[92m14:44:34 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:44:34 - LiteLLM:DEBUG[0m: token_counter.py:374 - messages in token_counter: None, text in token_counter: 
[92m14:44:34 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:44:34 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:34 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:34 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:44:34 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:44:34 - LiteLLM Router:INFO[0m: router.py:1331 - litellm.acompletion(model=ollama/deepseek-coder:6.7b)[32m 200 OK[0m
[92m14:44:34 - LiteLLM Router:DEBUG[0m: router.py:3892 - Async Response: <litellm.router.Router._acompletion_streaming_iterator.<locals>.FallbackStreamWrapper object at 0x7f4a04a38d90>
[92m14:44:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3273 - inside generator
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Sure', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-50178e62-6cd6-48c6-af10-eb7958093c41', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-50178e62-6cd6-48c6-af10-eb7958093c41', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ',', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0831d7d4-e620-4925-a202-614daaf2b9eb', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0831d7d4-e620-4925-a202-614daaf2b9eb', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' here', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8bd5d6f4-3c88-49cb-b3f0-afcf9f9f09f4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8bd5d6f4-3c88-49cb-b3f0-afcf9f9f09f4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' is', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-508bd4bf-b647-493c-b19b-b7940fe40050', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' is', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-508bd4bf-b647-493c-b19b-b7940fe40050', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' is', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2f0d8f7c-5f7d-444c-9191-f58f15b54b32', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2f0d8f7c-5f7d-444c-9191-f58f15b54b32', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' *', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-14fed0f2-7119-4a60-b2e7-ea0ff521dc3e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' *', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-14fed0f2-7119-4a60-b2e7-ea0ff521dc3e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' *', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'SE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2a5cc94d-46da-41ed-ae3b-29b180fcda16', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2a5cc94d-46da-41ed-ae3b-29b180fcda16', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ARCH', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c86472e8-f66f-4ace-b901-18a3654f4b1d', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c86472e8-f66f-4ace-b901-18a3654f4b1d', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '/', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f3c40b27-0c0a-41c4-8555-80b8a9052cf4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='/', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f3c40b27-0c0a-41c4-8555-80b8a9052cf4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='/', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'RE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4684cb85-34d1-4b04-bc49-a63a5f835152', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4684cb85-34d1-4b04-bc49-a63a5f835152', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'PL', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a1001d4c-9b6c-46d8-9470-b8ca2a75d27d', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a1001d4c-9b6c-46d8-9470-b8ca2a75d27d', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ACE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3e5d20fb-e316-4a3b-944f-556e04bc659f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3e5d20fb-e316-4a3b-944f-556e04bc659f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' block', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4cca8e95-8c78-4525-a967-1a8fe3f52eb4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' block', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4cca8e95-8c78-4525-a967-1a8fe3f52eb4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' block', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '*', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9a383d7a-564b-4e8f-b138-eeb1f58ca065', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='*', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9a383d7a-564b-4e8f-b138-eeb1f58ca065', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='*', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' for', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2329902a-af3e-4ba3-8bf1-105de34c8e46', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' for', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2329902a-af3e-4ba3-8bf1-105de34c8e46', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' for', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' creating', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fcfe8b78-170e-4323-81c4-9ff866176633', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' creating', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fcfe8b78-170e-4323-81c4-9ff866176633', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' creating', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' `', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e1f6727a-0800-4077-9638-8cbef8fd0a4c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e1f6727a-0800-4077-9638-8cbef8fd0a4c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-793f7abc-3be1-4ef6-a796-fb6072754745', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-793f7abc-3be1-4ef6-a796-fb6072754745', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-92a284c8-ff67-4b9d-94e2-78ddb6c1308f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-92a284c8-ff67-4b9d-94e2-78ddb6c1308f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'py', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e3ef8313-562f-471d-b474-1e4ee3ecc05c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e3ef8313-562f-471d-b474-1e4ee3ecc05c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '`', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b0ae112e-a4d7-4068-93f8-27349e0070b0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b0ae112e-a4d7-4068-93f8-27349e0070b0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' with', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-58c088ba-4047-4e58-8136-b5a30da07f78', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-58c088ba-4047-4e58-8136-b5a30da07f78', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4212fd29-cab6-46d8-a0bb-40bd1e63e3e3', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4212fd29-cab6-46d8-a0bb-40bd1e63e3e3', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' function', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4b1cc4b4-ef3b-46f8-8cc2-02f9c51321f8', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4b1cc4b4-ef3b-46f8-8cc2-02f9c51321f8', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' called', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8c4b770f-f2c4-41dd-9e92-b2c2c28c289e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' called', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8c4b770f-f2c4-41dd-9e92-b2c2c28c289e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' called', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1a2ac6a5-9dfe-40d0-9b34-a8fa79a42050', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1a2ac6a5-9dfe-40d0-9b34-a8fa79a42050', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '():', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a3aec33b-82de-4944-b0db-ce6f19c0720a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a3aec33b-82de-4944-b0db-ce6f19c0720a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-53ce05f3-d674-4eb6-9dc5-0850843c2b80', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-53ce05f3-d674-4eb6-9dc5-0850843c2b80', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-15dde8d1-0b38-4d1d-9a3e-0d9812f110a2', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-15dde8d1-0b38-4d1d-9a3e-0d9812f110a2', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8eb3de95-c4d6-421d-9edd-c5e7a4772533', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8eb3de95-c4d6-421d-9edd-c5e7a4772533', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'python', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-293f071a-61f9-4f1e-b3f1-c118fe170503', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-293f071a-61f9-4f1e-b3f1-c118fe170503', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c2df4dab-b429-43cc-801d-b873853cd28a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c2df4dab-b429-43cc-801d-b873853cd28a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fa7e7a66-4307-4c08-895b-cf10dec9cab7', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fa7e7a66-4307-4c08-895b-cf10dec9cab7', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7c22af3d-afd3-44da-a66f-f6465a903562', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7c22af3d-afd3-44da-a66f-f6465a903562', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-436bbf9e-48f7-4858-aaf5-f36fba3cf8c1', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-436bbf9e-48f7-4858-aaf5-f36fba3cf8c1', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6108fa96-7439-4739-b4f1-047c9066773c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6108fa96-7439-4739-b4f1-047c9066773c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' SE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ca6e599e-20ce-4931-ab71-4caab9edd6a5', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ca6e599e-20ce-4931-ab71-4caab9edd6a5', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ARCH', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:38 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8efd0c31-9035-48f8-9a2b-af1e97520bdc', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:38 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8efd0c31-9035-48f8-9a2b-af1e97520bdc', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-653117c6-d62c-470e-8f54-6da8fd67265a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-653117c6-d62c-470e-8f54-6da8fd67265a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '======', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ae39f89a-1a3e-4053-9d7d-fc544355886b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ae39f89a-1a3e-4053-9d7d-fc544355886b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '=', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4d8672eb-b4d8-48a2-8026-5aae275f9861', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4d8672eb-b4d8-48a2-8026-5aae275f9861', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0afd58dd-6a2f-443f-bddc-78dac0ac3789', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0afd58dd-6a2f-443f-bddc-78dac0ac3789', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '#', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a49519d3-72d7-446f-88f4-aea068730ddf', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='#', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a49519d3-72d7-446f-88f4-aea068730ddf', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='#', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' hell', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cbaabcc7-af61-4987-8df0-c7298685df11', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' hell', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cbaabcc7-af61-4987-8df0-c7298685df11', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' hell', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'o', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e8eb7c00-d6ef-4f5b-a548-5a97db220101', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='o', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e8eb7c00-d6ef-4f5b-a548-5a97db220101', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='o', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:39 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d923253a-d248-478f-94dc-0ca791ec8786', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:39 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d923253a-d248-478f-94dc-0ca791ec8786', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'py', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-906627b7-c8d8-4972-8796-7aa99c9ea70b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-906627b7-c8d8-4972-8796-7aa99c9ea70b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b894fbfb-6b40-4647-bee5-e9a31a4f3e4e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b894fbfb-6b40-4647-bee5-e9a31a4f3e4e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'def', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8f1a0465-b878-4824-b000-7907dda1cc76', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8f1a0465-b878-4824-b000-7907dda1cc76', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ee155c95-fe60-40fa-b545-ecc92e9cb460', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ee155c95-fe60-40fa-b545-ecc92e9cb460', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '():', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-12a80694-fe6a-4169-859f-866a878f409f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-12a80694-fe6a-4169-859f-866a878f409f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-54930d1f-ad06-4272-bccc-8c5af3629a2b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-54930d1f-ad06-4272-bccc-8c5af3629a2b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '   ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:40 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b8bcb702-a8a1-470d-8622-45c7f263abe3', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:40 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b8bcb702-a8a1-470d-8622-45c7f263abe3', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' "', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fff41fa1-4374-4190-a4d0-3dc24b19f7e0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fff41fa1-4374-4190-a4d0-3dc24b19f7e0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-110c2fae-9dd2-410d-86da-5340ec8e8697', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-110c2fae-9dd2-410d-86da-5340ec8e8697', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': " '", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7b3bfcf7-5b07-4752-b165-e448b32f8e9a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=" '", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7b3bfcf7-5b07-4752-b165-e448b32f8e9a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=" '", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3ee33a66-9c49-4741-b2ab-8481cfd22ac6', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3ee33a66-9c49-4741-b2ab-8481cfd22ac6', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5f1d8808-2c6b-4bcb-9604-afb92852c789', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5f1d8808-2c6b-4bcb-9604-afb92852c789', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a94431c5-5d3f-4bbf-acfa-c932ebc0a10f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a94431c5-5d3f-4bbf-acfa-c932ebc0a10f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:41 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-01772cf6-7ab7-434b-b64f-557c4c12f4ff', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:41 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-01772cf6-7ab7-434b-b64f-557c4c12f4ff', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d245c5a8-c334-44db-882e-b164316a56c1', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d245c5a8-c334-44db-882e-b164316a56c1', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-81768cd1-78ba-4b6b-862b-23909d06916b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-81768cd1-78ba-4b6b-862b-23909d06916b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\'"', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-12e098e1-7488-4e9c-b796-a23eae791a1d', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\'"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-12e098e1-7488-4e9c-b796-a23eae791a1d', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\'"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d0e0142f-1514-43ab-a426-1b503f8128a1', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d0e0142f-1514-43ab-a426-1b503f8128a1', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '   ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-97421536-8ef1-4874-8d2a-c4ab1611d4bd', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-97421536-8ef1-4874-8d2a-c4ab1611d4bd', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cfcb0818-34a5-4bc6-8a02-bc70f72e45eb', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cfcb0818-34a5-4bc6-8a02-bc70f72e45eb', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '("', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:42 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-58f505c1-3f5d-4a3a-8543-3ae6960dab8a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:42 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-58f505c1-3f5d-4a3a-8543-3ae6960dab8a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e132a372-a148-4097-9ed3-bd0ab0cb220c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e132a372-a148-4097-9ed3-bd0ab0cb220c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f5997761-efb4-4516-bd58-5ec99b980511', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f5997761-efb4-4516-bd58-5ec99b980511', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9e97b257-a5e2-476d-bd13-5556ac7e2de3', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9e97b257-a5e2-476d-bd13-5556ac7e2de3', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a634fe2a-df36-4544-88ce-2a3d7c54e58b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a634fe2a-df36-4544-88ce-2a3d7c54e58b', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-aed7aa6e-2464-4572-ba84-26ded85f3ef4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-aed7aa6e-2464-4572-ba84-26ded85f3ef4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6f95263b-2b9b-477f-8dfd-559890e5c0b4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6f95263b-2b9b-477f-8dfd-559890e5c0b4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '")', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7e83c632-7dd7-470b-ad89-d5792d1c0ee4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7e83c632-7dd7-470b-ad89-d5792d1c0ee4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:43 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-10b6f51d-062e-408d-bf76-860285936eb4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:43 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-10b6f51d-062e-408d-bf76-860285936eb4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-56abaf7d-4ab0-4492-ab8d-a5fa30ddae90', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-56abaf7d-4ab0-4492-ab8d-a5fa30ddae90', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-66f0e01d-e99e-4f4a-a4c5-4e45eae392b6', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-66f0e01d-e99e-4f4a-a4c5-4e45eae392b6', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c7ce0a41-2ea4-4e8c-a0c5-6c51a4a63c7c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c7ce0a41-2ea4-4e8c-a0c5-6c51a4a63c7c', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' RE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a84a7e4d-33eb-4d3a-bb93-b63d03643ff2', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a84a7e4d-33eb-4d3a-bb93-b63d03643ff2', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'PL', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5068f0d1-2a16-4445-9655-940536ae0a6e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5068f0d1-2a16-4445-9655-940536ae0a6e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ACE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-90b6293e-94e4-4bdb-b113-7d726d97cc44', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-90b6293e-94e4-4bdb-b113-7d726d97cc44', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:44 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-53615b56-3d6a-4ed0-99ff-9eb91427413e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:44 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-53615b56-3d6a-4ed0-99ff-9eb91427413e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e2c1e1b9-34f4-4793-8bc6-c2c15257777e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e2c1e1b9-34f4-4793-8bc6-c2c15257777e', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a95c2ec4-defe-4ff7-9b72-6470be9638a8', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a95c2ec4-defe-4ff7-9b72-6470be9638a8', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5e81f127-2613-4729-a340-5321cd718a84', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5e81f127-2613-4729-a340-5321cd718a84', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'This', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ae9b7f19-dfab-4df8-822b-ea273ee964d0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='This', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ae9b7f19-dfab-4df8-822b-ea273ee964d0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='This', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' block', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d7013cec-7ae9-4814-ac48-c6fc7a05c3de', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' block', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d7013cec-7ae9-4814-ac48-c6fc7a05c3de', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' block', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' will', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5b55d16b-c2e1-45b3-b4a8-00383ace7dc0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' will', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5b55d16b-c2e1-45b3-b4a8-00383ace7dc0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' will', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' create', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-91cb9b61-1a7e-4191-bcf3-00310ce71a95', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-91cb9b61-1a7e-4191-bcf3-00310ce71a95', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:45 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fbd5dccc-268d-4ca6-9031-42d6bf5ca3bb', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:45 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fbd5dccc-268d-4ca6-9031-42d6bf5ca3bb', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' new', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-efff914f-342a-4b19-9a87-82bea1761e35', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' new', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-efff914f-342a-4b19-9a87-82bea1761e35', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' new', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' file', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d4f58b6f-a57b-41ae-a03a-caf634674ea1', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' file', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d4f58b6f-a57b-41ae-a03a-caf634674ea1', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' file', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' `', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d1004385-6e16-4da6-890e-bb34e8aea8a0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d1004385-6e16-4da6-890e-bb34e8aea8a0', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-16106933-e224-4b0c-9c66-44bed5098cec', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-16106933-e224-4b0c-9c66-44bed5098cec', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9af14861-49e2-4a2e-aec6-3183b7d1fc86', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9af14861-49e2-4a2e-aec6-3183b7d1fc86', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'py', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ef7435fd-66c7-4bc4-8bc2-e504c8978c71', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ef7435fd-66c7-4bc4-8bc2-e504c8978c71', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '`', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:46 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-72049b15-91ce-438c-96d2-e846594ac28a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:46 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-72049b15-91ce-438c-96d2-e846594ac28a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' with', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c3494b9a-62dc-4553-8925-1e040ee92cfd', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c3494b9a-62dc-4553-8925-1e040ee92cfd', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' the', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-07fb24ee-48bb-46b4-862b-48b038b15719', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' the', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-07fb24ee-48bb-46b4-862b-48b038b15719', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' the', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' function', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7aaa77f9-1237-4902-9e5b-0b6f339947ff', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7aaa77f9-1237-4902-9e5b-0b6f339947ff', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' `', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-95b7d9d4-a714-40ee-9260-833ab451c2bb', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-95b7d9d4-a714-40ee-9260-833ab451c2bb', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'g', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0bfefb04-9bc0-4052-928a-a455a2715131', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='g', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0bfefb04-9bc0-4052-928a-a455a2715131', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='g', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'reet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d885169b-7742-4b67-8d91-cb7f8eda8885', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='reet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d885169b-7742-4b67-8d91-cb7f8eda8885', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='reet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '()', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-338af195-aff6-41bd-a65f-62363112e094', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-338af195-aff6-41bd-a65f-62363112e094', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '`', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:47 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-69855196-3e22-4264-9357-6ed6ba7c29d3', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:47 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-69855196-3e22-4264-9357-6ed6ba7c29d3', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' defined', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8acdc247-b587-4a65-a90c-5ed4aa07e462', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' defined', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8acdc247-b587-4a65-a90c-5ed4aa07e462', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' defined', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' in', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a2260367-fc8b-4710-a7cd-16fcbe25b244', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' in', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a2260367-fc8b-4710-a7cd-16fcbe25b244', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' in', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' it', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d3d70b87-a3dd-4069-8901-4a5e773d9acd', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' it', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d3d70b87-a3dd-4069-8901-4a5e773d9acd', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' it', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a6739509-35f3-49ac-8a06-8f907a8ca6f5', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a6739509-35f3-49ac-8a06-8f907a8ca6f5', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' The', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-60d9c7e1-f268-43b2-acaf-2d4fb989f74f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' The', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-60d9c7e1-f268-43b2-acaf-2d4fb989f74f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' The', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' user', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-81925aaf-c7a4-4cb2-8d17-56b26606ab0f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' user', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-81925aaf-c7a4-4cb2-8d17-56b26606ab0f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' user', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' can', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:48 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1a302ec4-cdb7-4ce4-aacc-c47c6bf42204', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:48 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1a302ec4-cdb7-4ce4-aacc-c47c6bf42204', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' then', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-28fdb6e8-204d-4164-918c-a109d375231a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' then', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-28fdb6e8-204d-4164-918c-a109d375231a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' then', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' add', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-68292eda-b1cc-439b-bb13-f1d2ca90b16d', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' add', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-68292eda-b1cc-439b-bb13-f1d2ca90b16d', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' add', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' this', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ebedd2e9-9de8-4954-8f76-4d8272123e92', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' this', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ebedd2e9-9de8-4954-8f76-4d8272123e92', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' this', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' file', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-bdb6b6bb-cf51-40f0-9c79-5a3eb0880aa2', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' file', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-bdb6b6bb-cf51-40f0-9c79-5a3eb0880aa2', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' file', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' to', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:49 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7e35c27e-7953-443d-bf05-709ae97a18aa', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' to', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:49 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7e35c27e-7953-443d-bf05-709ae97a18aa', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' to', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' their', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-237e0d32-620b-4dbf-8bf4-68909e8ae366', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' their', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-237e0d32-620b-4dbf-8bf4-68909e8ae366', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' their', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' git', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-341e853a-de39-4595-9989-5c8fb5f9b1b2', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' git', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-341e853a-de39-4595-9989-5c8fb5f9b1b2', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' git', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' repository', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d9cbbe25-039a-4d6a-85b7-953609fab99a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' repository', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d9cbbe25-039a-4d6a-85b7-953609fab99a', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' repository', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' and', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-425701df-507f-4070-ab8e-603e52be3e24', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' and', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-425701df-507f-4070-ab8e-603e52be3e24', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' and', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' commit', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-612949dc-aa7b-4e5b-81a2-3607e33adff4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' commit', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-612949dc-aa7b-4e5b-81a2-3607e33adff4', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' commit', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' the', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8ec6a5a7-6a7e-47b7-8e3a-255dd96963be', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' the', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8ec6a5a7-6a7e-47b7-8e3a-255dd96963be', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' the', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' changes', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:51 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-44cd6092-3ce4-4903-90b6-85c3d83013a7', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' changes', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:51 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-44cd6092-3ce4-4903-90b6-85c3d83013a7', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' changes', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:52 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:52 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-64cee644-2609-4be0-a6b5-494ad9df9900', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:52 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-64cee644-2609-4be0-a6b5-494ad9df9900', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:52 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:44:52 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6d8877c1-19e6-45bd-89d1-668542567734', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:52 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6d8877c1-19e6-45bd-89d1-668542567734', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:52 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '', 'is_finished': True, 'finish_reason': 'stop', 'usage': {'prompt_tokens': 2999, 'completion_tokens': 127, 'total_tokens': 3126}}
[92m14:44:52 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5cbed0db-072b-4c12-a8ea-a3c8c2a0263f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=127, prompt_tokens=2999, total_tokens=3126, completion_tokens_details=None, prompt_tokens_details=None))
[92m14:44:52 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5cbed0db-072b-4c12-a8ea-a3c8c2a0263f', created=1770561874, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:44:52 - LiteLLM:DEBUG[0m: litellm_logging.py:1603 - Logging Details LiteLLM-Success Call: Cache_hit=False
[92m14:44:52 - LiteLLM:DEBUG[0m: litellm_logging.py:1632 - Logging Details LiteLLM-Success Call streaming complete
[92m14:44:52 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:52 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:44:52 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:349 - Logging Details LiteLLM-Async Success Call, cache_hit=False
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:349 - Async success callbacks: Got a complete streaming response
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:52 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:52 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:44:52 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:44:52 - LiteLLM:DEBUG[0m: litellm_logging.py:2172 - Model=deepseek-coder:6.7b; cost=0.0
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:44:52 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.sync_deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:151 - in RouterBudgetLimiting.async_log_success_event
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:167 - Not running _PROXY_VirtualKeyModelMaxBudgetLimiter.async_log_success_event because user_api_key_model_max_budget is None or empty. `user_api_key_model_max_budget`={}
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - INSIDE parallel request limiter ASYNC SUCCESS LOGGING
[92m14:44:52 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 3126, 'current_rpm': 0}, precise_minute: 2026-02-08-14-44
[92m14:44:52 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 3126, 'current_rpm': 0}, precise_minute: 2026-02-08-14-44
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:44:52 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:46:28 - LiteLLM Proxy:DEBUG[0m: model_checks.py:121 - ALL KEY MODELS - 0
[92m14:46:28 - LiteLLM Proxy:DEBUG[0m: model_checks.py:153 - ALL TEAM MODELS - 0
INFO:     127.0.0.1:52200 - "GET /v1/models HTTP/1.1" 200 OK
[92m14:46:28 - LiteLLM Proxy:DEBUG[0m: model_checks.py:121 - ALL KEY MODELS - 0
[92m14:46:28 - LiteLLM Proxy:DEBUG[0m: model_checks.py:153 - ALL TEAM MODELS - 0
INFO:     127.0.0.1:52206 - "GET /v1/models HTTP/1.1" 200 OK
[92m14:46:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m14:46:37 - LiteLLM Proxy:DEBUG[0m: common_request_processing.py:388 - Request received by LiteLLM:
{
    "messages": [
        {
            "role": "system",
            "content": "Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven't been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user's project.\n\nUse the appropriate shell based on the user's system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        },
        {
            "role": "user",
            "content": "Change get_factorial() to use math.factorial"
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "Refactor hello() into its own file."
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "I switched to a new code base. Please don't consider the above files or try to edit them any longer."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "I am not sharing any files that you can edit yet."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "Create hello.py with a function greet() that prints 'Hello from Lodestar'"
        },
        {
            "role": "system",
            "content": "# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        }
    ],
    "model": "gpt-3.5-turbo",
    "stream": true,
    "temperature": 0
}
[92m14:46:37 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:664 - Request Headers: Headers({'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10579'})
[92m14:46:37 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:670 - receiving data: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10579'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10579'}}}
[92m14:46:37 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:839 - [PROXY] returned data from litellm_pre_call_utils: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10579'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10579'}}, 'metadata': {'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10579'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': ''}}
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:349 - Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510>, <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0>, <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0>]
[92m14:46:37 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {}
[92m14:46:37 - LiteLLM Proxy:DEBUG[0m: utils.py:1025 - Inside Proxy Logging Pre-call hook!
[92m14:46:37 - LiteLLM Proxy:DEBUG[0m: max_budget_limiter.py:23 - Inside Max Budget Limiter Pre-Call Hook
[92m14:46:37 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - Inside Max Parallel Request Pre-Call Hook
[92m14:46:37 - LiteLLM Proxy:INFO[0m: parallel_request_limiter.py:68 - Current Usage of key in this minute: None
[92m14:46:37 - LiteLLM Proxy:DEBUG[0m: cache_control_check.py:27 - Inside Cache Control Check Pre-Call Hook
[92m14:46:37 - LiteLLM Router:DEBUG[0m: router.py:3959 - Inside async function with retries.
[92m14:46:37 - LiteLLM Router:DEBUG[0m: router.py:3979 - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x7f4a04698690>>, num_retries - 3
[92m14:46:37 - LiteLLM Router:DEBUG[0m: router.py:6621 - initial list of deployments: [{'model_name': 'gpt-3.5-turbo', 'litellm_params': {'api_base': 'http://192.168.120.211:11434', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'ollama/deepseek-coder:6.7b'}, 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}}]
[92m14:46:37 - LiteLLM Router:DEBUG[0m: cooldown_handlers.py:342 - retrieve cooldown models: []
[92m14:46:37 - LiteLLM Router:DEBUG[0m: router.py:6688 - async cooldown deployments: []
[92m14:46:37 - LiteLLM Router:DEBUG[0m: router.py:6691 - cooldown_deployments: []
[92m14:46:37 - LiteLLM Router:DEBUG[0m: router.py:7055 - cooldown deployments: []
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:349 - [92mRequest to litellm:[0m
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:349 - [92mlitellm.acompletion(api_base='http://192.168.120.211:11434', use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='ollama/deepseek-coder:6.7b', messages=[{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], caching=False, client=None, temperature=0, proxy_server_request={'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10579'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, secret_fields={'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10579'}}, metadata={'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '10579'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': '', 'model_group': 'gpt-3.5-turbo', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'ollama/deepseek-coder:6.7b', 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, 'api_base': 'http://192.168.120.211:11434', 'deployment_model_name': 'gpt-3.5-turbo', 'caching_groups': None}, litellm_call_id='52542c46-964e-4936-a46d-7bfdb734ad0e', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x7f4a01c83fd0>, stream=True, litellm_trace_id='3bb71502-cdad-4d3b-89ab-588255505bcf', model_info={'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, timeout=600, max_retries=0)[0m
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:349 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
[92m14:46:37 - LiteLLM:DEBUG[0m: caching_handler.py:213 - CACHE RESULT: None
[92m14:46:37 - LiteLLM:INFO[0m: utils.py:3260 - 
LiteLLM completion() model= deepseek-coder:6.7b; provider = ollama
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:3263 - 
LiteLLM: Params passed to completion() {'model': 'deepseek-coder:6.7b', 'functions': None, 'function_call': None, 'temperature': 0, 'top_p': None, 'n': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'thinking': None, 'web_search_options': None}
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:3266 - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:349 - Final returned optional params: {'temperature': 0, 'stream': True}
[92m14:46:37 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:46:37 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:46:37 - LiteLLM:DEBUG[0m: litellm_logging.py:930 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
http://192.168.120.211:11434/api/generate \
-d '{'model': 'deepseek-coder:6.7b', 'prompt': '### System:\nAct as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n### User:\nChange get_factorial() to use math.factorial\n\n### Assistant:\nTo make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n\n### User:\nRefactor hello() into its own file.\n\n### Assistant:\nTo make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n\n### User:\nI switched to a new code base. Please don\'t consider the above files or try to edit them any longer.\n\n### Assistant:\nOk.\n\n### User:\nI am not sharing any files that you can edit yet.\n\n### Assistant:\nOk.\n\n### User:\nCreate hello.py with a function greet() that prints \'Hello from Lodestar\'\n\n### System:\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n', 'options': {'temperature': 0}, 'stream': True, 'images': []}'
[0m

[92m14:46:54 - LiteLLM:DEBUG[0m: utils.py:349 - RAW RESPONSE:
first stream response received


[92m14:46:54 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:46:54 - LiteLLM:DEBUG[0m: token_counter.py:374 - messages in token_counter: None, text in token_counter: 
[92m14:46:54 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:46:54 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:46:54 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:46:54 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:46:54 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:46:54 - LiteLLM Router:INFO[0m: router.py:1331 - litellm.acompletion(model=ollama/deepseek-coder:6.7b)[32m 200 OK[0m
[92m14:46:54 - LiteLLM Router:DEBUG[0m: router.py:3892 - Async Response: <litellm.router.Router._acompletion_streaming_iterator.<locals>.FallbackStreamWrapper object at 0x7f4a04ceef10>
[92m14:46:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3273 - inside generator
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Sure', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8606bdf5-7d7a-455e-a884-57025e65363b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8606bdf5-7d7a-455e-a884-57025e65363b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
INFO:     127.0.0.1:47124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ',', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1e024464-d606-4b1c-b63b-9bda9e55f539', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1e024464-d606-4b1c-b63b-9bda9e55f539', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' here', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1583a339-7e67-43cc-8862-23d16a5f4138', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1583a339-7e67-43cc-8862-23d16a5f4138', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': "'", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d3a9761f-abd7-421b-8ceb-76c4ad68a37a', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="'", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d3a9761f-abd7-421b-8ceb-76c4ad68a37a', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="'", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 's', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-dbbb6b67-3ffd-4125-bf80-a9ab62edf12d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='s', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-dbbb6b67-3ffd-4125-bf80-a9ab62edf12d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='s', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' how', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1e1b8f98-abe4-4316-9e9c-a4ef1b04ff74', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' how', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1e1b8f98-abe4-4316-9e9c-a4ef1b04ff74', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' how', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' you', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c671e791-ca65-447a-9719-4ebd771129ce', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c671e791-ca65-447a-9719-4ebd771129ce', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' can', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-12c38765-1d43-4083-9a5e-03e2c7663c1b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-12c38765-1d43-4083-9a5e-03e2c7663c1b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' create', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-33ea8c33-39cd-47d1-a667-7d88747bae41', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-33ea8c33-39cd-47d1-a667-7d88747bae41', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' `', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4a34d69b-88a6-4ff4-8064-63438c84da4f', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4a34d69b-88a6-4ff4-8064-63438c84da4f', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a7fc3d02-6e26-43d1-9bab-868b814192ee', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a7fc3d02-6e26-43d1-9bab-868b814192ee', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-db249581-a99d-4223-9b6c-e97b62c5ec9f', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-db249581-a99d-4223-9b6c-e97b62c5ec9f', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'py', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9fd10fb7-6912-42f1-8177-c3796db2d8f4', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9fd10fb7-6912-42f1-8177-c3796db2d8f4', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '`', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ee99b3ab-2c1b-4250-bf33-1bc4a77d5155', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ee99b3ab-2c1b-4250-bf33-1bc4a77d5155', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' with', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-bc2f54dc-d96c-40d4-988b-ec24307b450e', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-bc2f54dc-d96c-40d4-988b-ec24307b450e', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-606a48d2-e110-47b6-9a52-817f6b09d219', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-606a48d2-e110-47b6-9a52-817f6b09d219', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' function', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-79690eb3-0095-42ba-ac51-6218b57f0799', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-79690eb3-0095-42ba-ac51-6218b57f0799', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1a870efb-9cca-4e48-a75f-419ad03cc1fd', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1a870efb-9cca-4e48-a75f-419ad03cc1fd', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '()', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c9b6248d-9ce1-4fb3-bdbf-609edf2c06b1', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c9b6248d-9ce1-4fb3-bdbf-609edf2c06b1', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' that', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-38afe571-ca77-4495-97f0-9395bfacb853', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' that', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-38afe571-ca77-4495-97f0-9395bfacb853', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' that', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' prints', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-768a00db-0175-403d-9b08-6b70c3d1a874', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' prints', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-768a00db-0175-403d-9b08-6b70c3d1a874', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' prints', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': " '", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-756d0d98-6ecc-4bd9-b42a-f169fc385f28', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=" '", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-756d0d98-6ecc-4bd9-b42a-f169fc385f28', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=" '", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-dda7f6d5-f0d8-4855-ad5a-bdd1c80cfed0', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-dda7f6d5-f0d8-4855-ad5a-bdd1c80cfed0', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fbbd38a7-207c-442e-bd69-62aeafff74ba', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fbbd38a7-207c-442e-bd69-62aeafff74ba', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0e0d89b4-f2cb-41ae-8846-c578069457dc', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0e0d89b4-f2cb-41ae-8846-c578069457dc', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f0fee3ef-bc6f-418a-bf39-7d871b9b3dc2', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f0fee3ef-bc6f-418a-bf39-7d871b9b3dc2', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f48b6c70-1c6f-48c1-9658-caba67738f11', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f48b6c70-1c6f-48c1-9658-caba67738f11', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2893cba5-b329-4f17-b097-f0925617b140', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2893cba5-b329-4f17-b097-f0925617b140', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': "':", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-339fef50-8412-4a98-b9e1-8fd4708aa049', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="':", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-339fef50-8412-4a98-b9e1-8fd4708aa049', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="':", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-098f7fe9-8329-4966-aa19-ef27d71e8775', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-098f7fe9-8329-4966-aa19-ef27d71e8775', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f00d40bd-3cd9-4041-aca4-9203f07c7662', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f00d40bd-3cd9-4041-aca4-9203f07c7662', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-503bb3de-a604-491d-9b55-73720f7fcbc8', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-503bb3de-a604-491d-9b55-73720f7fcbc8', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'python', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-867e9749-71df-4a67-af01-5d847954ae3e', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-867e9749-71df-4a67-af01-5d847954ae3e', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2284d898-7352-4a9e-8d80-2e8f555e35c7', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2284d898-7352-4a9e-8d80-2e8f555e35c7', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5cc1db0f-7e95-4cb6-871d-2dfcb1cc41b9', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5cc1db0f-7e95-4cb6-871d-2dfcb1cc41b9', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b325109f-50c3-4b79-b4b3-bb000f7001ec', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b325109f-50c3-4b79-b4b3-bb000f7001ec', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9b11cd66-d14b-48a0-8528-5dbdda43df0d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9b11cd66-d14b-48a0-8528-5dbdda43df0d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7b49a63b-56e7-474f-8e4e-661888db4d53', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7b49a63b-56e7-474f-8e4e-661888db4d53', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' SE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6dda4303-509c-4234-9f17-0c39626f1c56', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6dda4303-509c-4234-9f17-0c39626f1c56', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ARCH', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-91a01ff6-645f-4304-a18c-b5a5da97c7b6', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-91a01ff6-645f-4304-a18c-b5a5da97c7b6', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e46d60a4-1039-49fa-bac8-5e7c7b147bea', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e46d60a4-1039-49fa-bac8-5e7c7b147bea', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '======', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8a600cc8-1fc3-4223-b1b5-446474c3766b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8a600cc8-1fc3-4223-b1b5-446474c3766b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '=', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e5df4e50-b0f3-40fc-b2be-7c2b2e71e4b9', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e5df4e50-b0f3-40fc-b2be-7c2b2e71e4b9', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-09b24db2-ae6c-4afe-812e-c4655489f50b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-09b24db2-ae6c-4afe-812e-c4655489f50b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'def', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:46:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1cdefbf3-a13e-4672-84b8-e7ab4b9b5e8d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:46:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1cdefbf3-a13e-4672-84b8-e7ab4b9b5e8d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1126781a-4388-43e5-aeae-75c8fb2dadfd', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1126781a-4388-43e5-aeae-75c8fb2dadfd', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '():', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-133dbc15-b2a8-49f9-b1f7-28271d77daa8', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-133dbc15-b2a8-49f9-b1f7-28271d77daa8', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-04cb1652-a708-4a01-96dd-346bb5928982', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-04cb1652-a708-4a01-96dd-346bb5928982', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '   ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a34a8d96-7f08-44a6-8844-bc0546ab9cf2', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a34a8d96-7f08-44a6-8844-bc0546ab9cf2', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' "', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-51463eeb-c739-4207-89d7-e0c523d4e212', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-51463eeb-c739-4207-89d7-e0c523d4e212', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8a8305a5-1fee-4bf0-b1fb-51d712172318', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8a8305a5-1fee-4bf0-b1fb-51d712172318', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b241d166-b305-4cbc-b939-db92f56a9f0a', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b241d166-b305-4cbc-b939-db92f56a9f0a', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ca8a83e3-ad48-4519-914e-c389be5665a2', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ca8a83e3-ad48-4519-914e-c389be5665a2', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ing', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e8374c02-7f9c-4cd7-98b1-ab2c10bcda46', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ing', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e8374c02-7f9c-4cd7-98b1-ab2c10bcda46', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ing', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '"', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c21133ef-7132-4085-92f3-d4b177f0c7c4', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c21133ef-7132-4085-92f3-d4b177f0c7c4', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6077a113-1f95-4652-85c9-050e661b2f93', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6077a113-1f95-4652-85c9-050e661b2f93', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d36d3c53-5253-44d1-adeb-c5b7e1f01ff9', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d36d3c53-5253-44d1-adeb-c5b7e1f01ff9', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '   ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d3ec6890-da18-46b0-9d2f-0e114ae74c5c', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d3ec6890-da18-46b0-9d2f-0e114ae74c5c', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4d4401eb-80fa-4e4e-8da0-45bb03829eef', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4d4401eb-80fa-4e4e-8da0-45bb03829eef', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '("', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ad77a4ee-4e9e-4aee-90f6-b1cf744f2d7d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ad77a4ee-4e9e-4aee-90f6-b1cf744f2d7d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b52b5de7-51a4-4eb7-976a-93213b46636c', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b52b5de7-51a4-4eb7-976a-93213b46636c', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3e07837f-3c59-4e00-ba9d-d1c6bc80c03b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3e07837f-3c59-4e00-ba9d-d1c6bc80c03b', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-180b4409-199c-44fe-ba27-963695365a5f', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-180b4409-199c-44fe-ba27-963695365a5f', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b5af942d-7658-4273-84b2-3cc580bcb236', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b5af942d-7658-4273-84b2-3cc580bcb236', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e3dd8786-8e56-444c-a44d-9c76d134beca', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e3dd8786-8e56-444c-a44d-9c76d134beca', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ee9b29a4-1354-4f5b-a04f-16fcd26128c6', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ee9b29a4-1354-4f5b-a04f-16fcd26128c6', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '")', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fc64a7d9-ca1b-4172-b034-44f780457c89', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fc64a7d9-ca1b-4172-b034-44f780457c89', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-99b603bb-afd2-4c9c-9a86-eba17d12ae0e', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-99b603bb-afd2-4c9c-9a86-eba17d12ae0e', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e2fe8a43-c205-498f-9b8d-833c24f0a9a1', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e2fe8a43-c205-498f-9b8d-833c24f0a9a1', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-abe88355-3c7c-4a45-aeb0-9f14f8ba676d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-abe88355-3c7c-4a45-aeb0-9f14f8ba676d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e9a5f89d-4cae-4678-b262-2aefe25f825d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e9a5f89d-4cae-4678-b262-2aefe25f825d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' RE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0d91136e-9055-4b06-93bc-9d8b0cc2e84f', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0d91136e-9055-4b06-93bc-9d8b0cc2e84f', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'PL', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cc2d2b64-1796-4a64-952e-73ecc6e65ce7', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cc2d2b64-1796-4a64-952e-73ecc6e65ce7', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ACE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4d0888ec-a03f-494c-b458-7b665a79ac7d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4d0888ec-a03f-494c-b458-7b665a79ac7d', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2759d3ce-c924-415f-8f06-c4dcad92bf31', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2759d3ce-c924-415f-8f06-c4dcad92bf31', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e07393b1-cecb-41b2-9cf5-a9019b2f118e', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e07393b1-cecb-41b2-9cf5-a9019b2f118e', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-186e2c8b-f850-4d39-8e51-38547ec909ac', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-186e2c8b-f850-4d39-8e51-38547ec909ac', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '', 'is_finished': True, 'finish_reason': 'stop', 'usage': {'prompt_tokens': 2995, 'completion_tokens': 78, 'total_tokens': 3073}}
[92m14:47:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-bcf9f2f0-2655-4758-a0d2-42dba45358d5', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=78, prompt_tokens=2995, total_tokens=3073, completion_tokens_details=None, prompt_tokens_details=None))
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-bcf9f2f0-2655-4758-a0d2-42dba45358d5', created=1770562014, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:349 - Logging Details LiteLLM-Async Success Call, cache_hit=False
[92m14:47:04 - LiteLLM:DEBUG[0m: litellm_logging.py:1603 - Logging Details LiteLLM-Success Call: Cache_hit=False
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:349 - Async success callbacks: Got a complete streaming response
[92m14:47:04 - LiteLLM:DEBUG[0m: litellm_logging.py:1632 - Logging Details LiteLLM-Success Call streaming complete
[92m14:47:04 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:47:04 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:47:04 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:47:04 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:47:04 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:47:04 - LiteLLM:DEBUG[0m: litellm_logging.py:2172 - Model=deepseek-coder:6.7b; cost=0.0
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.sync_deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:151 - in RouterBudgetLimiting.async_log_success_event
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:167 - Not running _PROXY_VirtualKeyModelMaxBudgetLimiter.async_log_success_event because user_api_key_model_max_budget is None or empty. `user_api_key_model_max_budget`={}
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - INSIDE parallel request limiter ASYNC SUCCESS LOGGING
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 3073, 'current_rpm': 0}, precise_minute: 2026-02-08-14-47
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 3073, 'current_rpm': 0}, precise_minute: 2026-02-08-14-47
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:04 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: common_request_processing.py:388 - Request received by LiteLLM:
{
    "messages": [
        {
            "role": "system",
            "content": "Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven't been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user's project.\n\nUse the appropriate shell based on the user's system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        },
        {
            "role": "user",
            "content": "Change get_factorial() to use math.factorial"
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "Refactor hello() into its own file."
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "I switched to a new code base. Please don't consider the above files or try to edit them any longer."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "I am not sharing any files that you can edit yet."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "Create hello.py with a function greet() that prints 'Hello from Lodestar'"
        },
        {
            "role": "assistant",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    \"print a greeting\"\n\n    print(\"Hello from Lodestar\")\n>>>>>>> REPLACE\n```"
        },
        {
            "role": "user",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"
        },
        {
            "role": "system",
            "content": "# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        }
    ],
    "model": "gpt-3.5-turbo",
    "stream": true,
    "temperature": 0
}
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:664 - Request Headers: Headers({'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11108'})
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:670 - receiving data: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11108'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11108'}}}
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:839 - [PROXY] returned data from litellm_pre_call_utils: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11108'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11108'}}, 'metadata': {'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11108'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': ''}}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:349 - Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510>, <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0>, <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0>]
[92m14:47:04 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {}
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: utils.py:1025 - Inside Proxy Logging Pre-call hook!
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: max_budget_limiter.py:23 - Inside Max Budget Limiter Pre-Call Hook
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - Inside Max Parallel Request Pre-Call Hook
[92m14:47:04 - LiteLLM Proxy:INFO[0m: parallel_request_limiter.py:68 - Current Usage of key in this minute: {'current_requests': 0, 'current_tpm': 3073, 'current_rpm': 0}
[92m14:47:04 - LiteLLM Proxy:DEBUG[0m: cache_control_check.py:27 - Inside Cache Control Check Pre-Call Hook
[92m14:47:04 - LiteLLM Router:DEBUG[0m: router.py:3959 - Inside async function with retries.
[92m14:47:04 - LiteLLM Router:DEBUG[0m: router.py:3979 - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x7f4a04698690>>, num_retries - 3
[92m14:47:04 - LiteLLM Router:DEBUG[0m: router.py:6621 - initial list of deployments: [{'model_name': 'gpt-3.5-turbo', 'litellm_params': {'api_base': 'http://192.168.120.211:11434', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'ollama/deepseek-coder:6.7b'}, 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}}]
[92m14:47:04 - LiteLLM Router:DEBUG[0m: cooldown_handlers.py:342 - retrieve cooldown models: []
[92m14:47:04 - LiteLLM Router:DEBUG[0m: router.py:6688 - async cooldown deployments: []
[92m14:47:04 - LiteLLM Router:DEBUG[0m: router.py:6691 - cooldown_deployments: []
[92m14:47:04 - LiteLLM Router:DEBUG[0m: router.py:7055 - cooldown deployments: []
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:349 - [92mRequest to litellm:[0m
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:349 - [92mlitellm.acompletion(api_base='http://192.168.120.211:11434', use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='ollama/deepseek-coder:6.7b', messages=[{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], caching=False, client=None, temperature=0, proxy_server_request={'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11108'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, secret_fields={'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11108'}}, metadata={'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11108'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': '', 'model_group': 'gpt-3.5-turbo', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'ollama/deepseek-coder:6.7b', 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, 'api_base': 'http://192.168.120.211:11434', 'deployment_model_name': 'gpt-3.5-turbo', 'caching_groups': None}, litellm_call_id='1ed0f98d-f45c-4a52-a291-b785a892d590', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x7f4a00ad2810>, stream=True, litellm_trace_id='a92e0a0e-03fd-4d4b-a3ba-d3d86f4c92a7', model_info={'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, timeout=600, max_retries=0)[0m
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:349 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
[92m14:47:04 - LiteLLM:DEBUG[0m: caching_handler.py:213 - CACHE RESULT: None
[92m14:47:04 - LiteLLM:INFO[0m: utils.py:3260 - 
LiteLLM completion() model= deepseek-coder:6.7b; provider = ollama
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:3263 - 
LiteLLM: Params passed to completion() {'model': 'deepseek-coder:6.7b', 'functions': None, 'function_call': None, 'temperature': 0, 'top_p': None, 'n': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'thinking': None, 'web_search_options': None}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:3266 - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:349 - Final returned optional params: {'temperature': 0, 'stream': True}
[92m14:47:04 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:04 - LiteLLM:DEBUG[0m: litellm_logging.py:930 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
http://192.168.120.211:11434/api/generate \
-d '{'model': 'deepseek-coder:6.7b', 'prompt': '### System:\nAct as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n### User:\nChange get_factorial() to use math.factorial\n\n### Assistant:\nTo make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n\n### User:\nRefactor hello() into its own file.\n\n### Assistant:\nTo make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n\n### User:\nI switched to a new code base. Please don\'t consider the above files or try to edit them any longer.\n\n### Assistant:\nOk.\n\n### User:\nI am not sharing any files that you can edit yet.\n\n### Assistant:\nOk.\n\n### User:\nCreate hello.py with a function greet() that prints \'Hello from Lodestar\'\n\n### Assistant:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```\n\n### User:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```\n\n### System:\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n', 'options': {'temperature': 0}, 'stream': True, 'images': []}'
[0m

[92m14:47:23 - LiteLLM:DEBUG[0m: utils.py:349 - RAW RESPONSE:
first stream response received


[92m14:47:23 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:47:23 - LiteLLM:DEBUG[0m: token_counter.py:374 - messages in token_counter: None, text in token_counter: 
[92m14:47:23 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:23 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:23 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:23 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:47:23 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:47:23 - LiteLLM Router:INFO[0m: router.py:1331 - litellm.acompletion(model=ollama/deepseek-coder:6.7b)[32m 200 OK[0m
[92m14:47:23 - LiteLLM Router:DEBUG[0m: router.py:3892 - Async Response: <litellm.router.Router._acompletion_streaming_iterator.<locals>.FallbackStreamWrapper object at 0x7f4a00ad0cd0>
[92m14:47:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3273 - inside generator
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Sure', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1c28a2d3-bdfb-4329-b579-dbd9d6323232', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1c28a2d3-bdfb-4329-b579-dbd9d6323232', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
INFO:     127.0.0.1:47124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ',', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-148a8784-c122-4e7a-9c9f-6b5e9c977cf7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-148a8784-c122-4e7a-9c9f-6b5e9c977cf7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' here', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-73ca7ef2-104e-4b7f-8f75-2d74fa6c613c', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-73ca7ef2-104e-4b7f-8f75-2d74fa6c613c', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': "'", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d7a1966b-d396-4592-8b30-ecb6e497e1af', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="'", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d7a1966b-d396-4592-8b30-ecb6e497e1af', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="'", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 's', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-53945675-e418-4ed3-8b30-2d6173371631', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='s', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-53945675-e418-4ed3-8b30-2d6173371631', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='s', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' how', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5da7799a-f3b7-45ee-905e-3ec99e1d147b', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' how', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5da7799a-f3b7-45ee-905e-3ec99e1d147b', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' how', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' you', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1f4adad0-80eb-468d-a32b-c8848626dd13', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1f4adad0-80eb-468d-a32b-c8848626dd13', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' can', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e28f1417-9ce7-40b2-9c4a-16d3b5f787f8', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e28f1417-9ce7-40b2-9c4a-16d3b5f787f8', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' create', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fc741320-4772-4e20-9f2f-eaacd546d050', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fc741320-4772-4e20-9f2f-eaacd546d050', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' `', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-40a42dbf-e429-423e-9105-79ff840ad18a', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-40a42dbf-e429-423e-9105-79ff840ad18a', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f533f326-0e7e-4fe5-89f8-98f3c956f8bf', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f533f326-0e7e-4fe5-89f8-98f3c956f8bf', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2c146f19-1870-48aa-b41d-8434db457c4f', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2c146f19-1870-48aa-b41d-8434db457c4f', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'py', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8de3cd72-9521-44c3-954d-4cf13167d86e', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8de3cd72-9521-44c3-954d-4cf13167d86e', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '`', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ac9a7857-de3e-4192-afd4-481e4eb58002', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ac9a7857-de3e-4192-afd4-481e4eb58002', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' with', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-15ab7ecd-8852-4273-8469-6bd59fa098d2', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-15ab7ecd-8852-4273-8469-6bd59fa098d2', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1dc4d28d-f75f-442b-9702-1c3712e7f85d', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1dc4d28d-f75f-442b-9702-1c3712e7f85d', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' function', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3455fd94-2e96-4c14-8a46-a711e39d97ac', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3455fd94-2e96-4c14-8a46-a711e39d97ac', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7b4b47d7-f286-41dd-b36e-a0d9a5786779', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7b4b47d7-f286-41dd-b36e-a0d9a5786779', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '()', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2011b950-4d3a-42d0-b443-720dfc6f1c50', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2011b950-4d3a-42d0-b443-720dfc6f1c50', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' that', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fb2ab5fa-1024-435b-b0f8-ba270c20cf9f', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' that', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fb2ab5fa-1024-435b-b0f8-ba270c20cf9f', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' that', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' prints', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e7c726b3-8492-457a-a750-70a1357a8348', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' prints', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e7c726b3-8492-457a-a750-70a1357a8348', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' prints', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': " '", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d4b33f53-cd7e-4490-922e-fe802e05a42f', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=" '", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d4b33f53-cd7e-4490-922e-fe802e05a42f', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=" '", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ffd15287-01d3-4a4e-a59d-5d3bbd49b451', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ffd15287-01d3-4a4e-a59d-5d3bbd49b451', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7efbee2e-4d17-4e16-9c03-693d59496a09', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7efbee2e-4d17-4e16-9c03-693d59496a09', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c2bf5353-0bd8-47f5-bef6-393c1493f116', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c2bf5353-0bd8-47f5-bef6-393c1493f116', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-eaacde07-2837-4fcf-b6a9-1cfe02fcc530', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-eaacde07-2837-4fcf-b6a9-1cfe02fcc530', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4b89f844-04fd-4cf7-8e9d-b74244af85a7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4b89f844-04fd-4cf7-8e9d-b74244af85a7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8ab57fc8-7099-4f43-bc79-e85646f41ffb', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8ab57fc8-7099-4f43-bc79-e85646f41ffb', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': "':", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-40414c3f-15f6-484b-a1f7-83cb66be192e', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="':", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-40414c3f-15f6-484b-a1f7-83cb66be192e', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="':", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9c67849d-7a8d-46db-9e91-20a09c937917', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9c67849d-7a8d-46db-9e91-20a09c937917', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-36fe2e55-3054-4f8e-b18e-f4a4f5f1dfcc', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-36fe2e55-3054-4f8e-b18e-f4a4f5f1dfcc', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0ea5174d-71f0-4310-a2c3-66d329538ca2', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0ea5174d-71f0-4310-a2c3-66d329538ca2', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'python', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a4c826d7-4420-430e-ac1c-1daf7eb6d2e9', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a4c826d7-4420-430e-ac1c-1daf7eb6d2e9', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4c7b5f4b-823b-493f-b5ee-0f290e6ecb8b', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4c7b5f4b-823b-493f-b5ee-0f290e6ecb8b', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-31fb9171-0320-40f3-a34e-6461a51d68a7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-31fb9171-0320-40f3-a34e-6461a51d68a7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-97dfd77b-ace5-4482-a6da-ffe0565c8904', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-97dfd77b-ace5-4482-a6da-ffe0565c8904', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7a44f2cf-979d-4e2b-bcc3-b102a55687f4', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7a44f2cf-979d-4e2b-bcc3-b102a55687f4', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-773c783b-d0e2-4017-97e5-e5a55daef65d', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-773c783b-d0e2-4017-97e5-e5a55daef65d', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' SE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-25b5f5dc-ee08-4a17-a68b-498e7fef8e34', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-25b5f5dc-ee08-4a17-a68b-498e7fef8e34', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ARCH', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2275ff3e-0573-4956-9573-4a6691856213', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2275ff3e-0573-4956-9573-4a6691856213', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-dc6e119f-5b21-4ed9-b942-bf120a077d1f', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-dc6e119f-5b21-4ed9-b942-bf120a077d1f', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '======', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-14d8ccf4-9c1a-4e43-8c8e-4ddb5b4d6ad1', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-14d8ccf4-9c1a-4e43-8c8e-4ddb5b4d6ad1', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '=', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-bb341971-ed77-468b-ab06-b361548eb216', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-bb341971-ed77-468b-ab06-b361548eb216', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-27abc3de-f32a-49bc-b381-e95491a799d5', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-27abc3de-f32a-49bc-b381-e95491a799d5', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'def', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-00c185e7-fe43-4b28-b83e-699b681ffba7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-00c185e7-fe43-4b28-b83e-699b681ffba7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f560c0ad-453a-48a7-99d2-5e0bcc78d6ec', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f560c0ad-453a-48a7-99d2-5e0bcc78d6ec', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '():', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e11c124e-91ba-4ad2-9299-9b3fbf305083', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e11c124e-91ba-4ad2-9299-9b3fbf305083', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0bed08e2-6c07-4efa-8ae0-d971ef0559f6', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0bed08e2-6c07-4efa-8ae0-d971ef0559f6', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '   ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-44990f8e-c893-4d3f-841e-73ebf2e4785b', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-44990f8e-c893-4d3f-841e-73ebf2e4785b', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' "', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-201ac617-0c0a-44db-bdab-1c3ffea249fe', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-201ac617-0c0a-44db-bdab-1c3ffea249fe', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-22eecef9-5afa-4a86-a432-0a825f434f15', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-22eecef9-5afa-4a86-a432-0a825f434f15', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-90969729-2a31-4c23-90e4-83fa12dad2ae', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-90969729-2a31-4c23-90e4-83fa12dad2ae', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a8c44fd1-d22d-4ffa-82d3-894f59917f47', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a8c44fd1-d22d-4ffa-82d3-894f59917f47', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ing', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-36dc1b88-27b8-481b-946a-aed734d54dcd', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ing', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-36dc1b88-27b8-481b-946a-aed734d54dcd', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ing', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '"', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-84005bab-c5ee-45c1-9c1e-8a9d169dba21', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-84005bab-c5ee-45c1-9c1e-8a9d169dba21', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7e62b790-fb28-4f75-9a89-de33e27b14c8', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7e62b790-fb28-4f75-9a89-de33e27b14c8', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f7c74dec-e190-4838-bbc4-7588ffd82acc', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f7c74dec-e190-4838-bbc4-7588ffd82acc', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '   ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4c3c58b4-ffc5-490e-aa98-b30f41aee566', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4c3c58b4-ffc5-490e-aa98-b30f41aee566', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b56cc82c-6b24-4308-83d1-b26a8f68696e', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b56cc82c-6b24-4308-83d1-b26a8f68696e', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '("', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cfa5cf58-f4d4-47d3-8070-e29988f0b466', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cfa5cf58-f4d4-47d3-8070-e29988f0b466', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a1f056b7-68c9-4f3d-bfbe-0f13c7d6f64d', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a1f056b7-68c9-4f3d-bfbe-0f13c7d6f64d', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-72775896-054a-4da4-bb8e-52b68a05daba', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-72775896-054a-4da4-bb8e-52b68a05daba', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7c797511-a0cd-4ecb-8540-d6d09ab0568c', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:31 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7c797511-a0cd-4ecb-8540-d6d09ab0568c', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fc9c632b-8c04-41c2-b348-a1c78e7b3325', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fc9c632b-8c04-41c2-b348-a1c78e7b3325', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-12443e59-786d-4764-9d44-870fdfac2996', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-12443e59-786d-4764-9d44-870fdfac2996', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8fbba0e7-e32d-44b9-b2a0-b3739645d343', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8fbba0e7-e32d-44b9-b2a0-b3739645d343', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '")', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a8438fdb-7059-43c9-a683-3f16c79a1a71', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a8438fdb-7059-43c9-a683-3f16c79a1a71', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-80b36bb6-6461-4354-b217-c29f3679bc27', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-80b36bb6-6461-4354-b217-c29f3679bc27', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ea90a951-cfa9-46c6-93b0-f8d0ee18af0b', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ea90a951-cfa9-46c6-93b0-f8d0ee18af0b', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3eb6270b-eef1-44c3-836d-adb32760b359', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3eb6270b-eef1-44c3-836d-adb32760b359', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0529858d-3c7b-4ebd-a84e-6588b8cbf6b7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0529858d-3c7b-4ebd-a84e-6588b8cbf6b7', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' RE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8314aec2-f8f2-4a1f-8909-73c18c755468', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8314aec2-f8f2-4a1f-8909-73c18c755468', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'PL', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fd0e7509-77ac-425b-bee4-8b3c1405a46c', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fd0e7509-77ac-425b-bee4-8b3c1405a46c', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ACE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a587ccb3-20e6-44c2-b207-62baaa987888', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a587ccb3-20e6-44c2-b207-62baaa987888', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-50a1a5b3-9796-4133-a188-4628d89395f3', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-50a1a5b3-9796-4133-a188-4628d89395f3', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ef1cd493-9508-4d5c-9e2a-2276f4786c80', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ef1cd493-9508-4d5c-9e2a-2276f4786c80', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-669646a9-9ca1-4fd9-866f-f8de6eae8260', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-669646a9-9ca1-4fd9-866f-f8de6eae8260', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '', 'is_finished': True, 'finish_reason': 'stop', 'usage': {'prompt_tokens': 3149, 'completion_tokens': 78, 'total_tokens': 3227}}
[92m14:47:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-530e57bb-9699-4a7c-be65-c6bd3f0e7034', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=78, prompt_tokens=3149, total_tokens=3227, completion_tokens_details=None, prompt_tokens_details=None))
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-530e57bb-9699-4a7c-be65-c6bd3f0e7034', created=1770562043, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:349 - Logging Details LiteLLM-Async Success Call, cache_hit=False
[92m14:47:34 - LiteLLM:DEBUG[0m: litellm_logging.py:1603 - Logging Details LiteLLM-Success Call: Cache_hit=False
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:349 - Async success callbacks: Got a complete streaming response
[92m14:47:34 - LiteLLM:DEBUG[0m: litellm_logging.py:1632 - Logging Details LiteLLM-Success Call streaming complete
[92m14:47:34 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:47:34 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:47:34 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:47:34 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:47:34 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:47:34 - LiteLLM:DEBUG[0m: litellm_logging.py:2172 - Model=deepseek-coder:6.7b; cost=0.0
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:151 - in RouterBudgetLimiting.async_log_success_event
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:167 - Not running _PROXY_VirtualKeyModelMaxBudgetLimiter.async_log_success_event because user_api_key_model_max_budget is None or empty. `user_api_key_model_max_budget`={}
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - INSIDE parallel request limiter ASYNC SUCCESS LOGGING
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 6300, 'current_rpm': 1}, precise_minute: 2026-02-08-14-47
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 6300, 'current_rpm': 0}, precise_minute: 2026-02-08-14-47
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.sync_deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:47:34 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: common_request_processing.py:388 - Request received by LiteLLM:
{
    "messages": [
        {
            "role": "system",
            "content": "Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven't been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user's project.\n\nUse the appropriate shell based on the user's system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        },
        {
            "role": "user",
            "content": "Change get_factorial() to use math.factorial"
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "Refactor hello() into its own file."
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "I switched to a new code base. Please don't consider the above files or try to edit them any longer."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "I am not sharing any files that you can edit yet."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "Create hello.py with a function greet() that prints 'Hello from Lodestar'"
        },
        {
            "role": "assistant",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    \"print a greeting\"\n\n    print(\"Hello from Lodestar\")\n>>>>>>> REPLACE\n```"
        },
        {
            "role": "user",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"
        },
        {
            "role": "assistant",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    \"print a greeting\"\n\n    print(\"Hello from Lodestar\")\n>>>>>>> REPLACE\n```"
        },
        {
            "role": "user",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"
        },
        {
            "role": "system",
            "content": "# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        }
    ],
    "model": "gpt-3.5-turbo",
    "stream": true,
    "temperature": 0
}
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:664 - Request Headers: Headers({'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11637'})
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:670 - receiving data: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11637'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11637'}}}
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:839 - [PROXY] returned data from litellm_pre_call_utils: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11637'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11637'}}, 'metadata': {'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11637'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': ''}}
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:349 - Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510>, <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0>, <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0>]
[92m14:47:34 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {}
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: utils.py:1025 - Inside Proxy Logging Pre-call hook!
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: max_budget_limiter.py:23 - Inside Max Budget Limiter Pre-Call Hook
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - Inside Max Parallel Request Pre-Call Hook
[92m14:47:34 - LiteLLM Proxy:INFO[0m: parallel_request_limiter.py:68 - Current Usage of key in this minute: {'current_requests': 0, 'current_tpm': 6300, 'current_rpm': 1}
[92m14:47:34 - LiteLLM Proxy:DEBUG[0m: cache_control_check.py:27 - Inside Cache Control Check Pre-Call Hook
[92m14:47:34 - LiteLLM Router:DEBUG[0m: router.py:3959 - Inside async function with retries.
[92m14:47:34 - LiteLLM Router:DEBUG[0m: router.py:3979 - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x7f4a04698690>>, num_retries - 3
[92m14:47:34 - LiteLLM Router:DEBUG[0m: router.py:6621 - initial list of deployments: [{'model_name': 'gpt-3.5-turbo', 'litellm_params': {'api_base': 'http://192.168.120.211:11434', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'ollama/deepseek-coder:6.7b'}, 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}}]
[92m14:47:34 - LiteLLM Router:DEBUG[0m: cooldown_handlers.py:342 - retrieve cooldown models: []
[92m14:47:34 - LiteLLM Router:DEBUG[0m: router.py:6688 - async cooldown deployments: []
[92m14:47:34 - LiteLLM Router:DEBUG[0m: router.py:6691 - cooldown_deployments: []
[92m14:47:34 - LiteLLM Router:DEBUG[0m: router.py:7055 - cooldown deployments: []
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:349 - [92mRequest to litellm:[0m
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:349 - [92mlitellm.acompletion(api_base='http://192.168.120.211:11434', use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='ollama/deepseek-coder:6.7b', messages=[{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], caching=False, client=None, temperature=0, proxy_server_request={'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11637'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, secret_fields={'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11637'}}, metadata={'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '11637'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': '', 'model_group': 'gpt-3.5-turbo', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'ollama/deepseek-coder:6.7b', 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, 'api_base': 'http://192.168.120.211:11434', 'deployment_model_name': 'gpt-3.5-turbo', 'caching_groups': None}, litellm_call_id='ebb13cc1-332d-475b-9e30-a24325a10597', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x7f4a00b1f0d0>, stream=True, litellm_trace_id='e8d36118-7825-4023-b511-101b8b1b47cf', model_info={'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, timeout=600, max_retries=0)[0m
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:349 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
[92m14:47:34 - LiteLLM:DEBUG[0m: caching_handler.py:213 - CACHE RESULT: None
[92m14:47:34 - LiteLLM:INFO[0m: utils.py:3260 - 
LiteLLM completion() model= deepseek-coder:6.7b; provider = ollama
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:3263 - 
LiteLLM: Params passed to completion() {'model': 'deepseek-coder:6.7b', 'functions': None, 'function_call': None, 'temperature': 0, 'top_p': None, 'n': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'thinking': None, 'web_search_options': None}
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:3266 - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:349 - Final returned optional params: {'temperature': 0, 'stream': True}
[92m14:47:34 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:34 - LiteLLM:DEBUG[0m: litellm_logging.py:930 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
http://192.168.120.211:11434/api/generate \
-d '{'model': 'deepseek-coder:6.7b', 'prompt': '### System:\nAct as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n### User:\nChange get_factorial() to use math.factorial\n\n### Assistant:\nTo make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n\n### User:\nRefactor hello() into its own file.\n\n### Assistant:\nTo make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n\n### User:\nI switched to a new code base. Please don\'t consider the above files or try to edit them any longer.\n\n### Assistant:\nOk.\n\n### User:\nI am not sharing any files that you can edit yet.\n\n### Assistant:\nOk.\n\n### User:\nCreate hello.py with a function greet() that prints \'Hello from Lodestar\'\n\n### Assistant:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```\n\n### User:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```\n\n### Assistant:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```\n\n### User:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```\n\n### System:\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n', 'options': {'temperature': 0}, 'stream': True, 'images': []}'
[0m

[92m14:47:53 - LiteLLM:DEBUG[0m: utils.py:349 - RAW RESPONSE:
first stream response received


[92m14:47:53 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:47:53 - LiteLLM:DEBUG[0m: token_counter.py:374 - messages in token_counter: None, text in token_counter: 
[92m14:47:53 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:47:53 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:53 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:47:53 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:47:53 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:47:53 - LiteLLM Router:INFO[0m: router.py:1331 - litellm.acompletion(model=ollama/deepseek-coder:6.7b)[32m 200 OK[0m
[92m14:47:53 - LiteLLM Router:DEBUG[0m: router.py:3892 - Async Response: <litellm.router.Router._acompletion_streaming_iterator.<locals>.FallbackStreamWrapper object at 0x7f4a00b1f350>
[92m14:47:53 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3273 - inside generator
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Sure', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ebff6049-6aac-49b8-b994-b5ad9b98a52b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ebff6049-6aac-49b8-b994-b5ad9b98a52b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
INFO:     127.0.0.1:47124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ',', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f5210962-a470-45be-890f-5286fad4f6a7', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f5210962-a470-45be-890f-5286fad4f6a7', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' here', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a9f18635-9b95-4f35-a0bb-783a4fa954f1', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a9f18635-9b95-4f35-a0bb-783a4fa954f1', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': "'", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-89e4d3e0-971f-4cec-94fb-a84a99469280', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="'", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-89e4d3e0-971f-4cec-94fb-a84a99469280', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="'", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 's', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2b16919b-a1b1-4812-88eb-c1bc8efd141c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='s', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2b16919b-a1b1-4812-88eb-c1bc8efd141c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='s', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' how', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4f6cad39-6599-44f8-8cdb-c4355efda524', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' how', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4f6cad39-6599-44f8-8cdb-c4355efda524', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' how', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' you', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:53 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8b101e94-ddd1-4060-91a4-ba0a4855e495', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:53 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8b101e94-ddd1-4060-91a4-ba0a4855e495', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' can', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fedb814f-7903-410f-acfa-fb64152b6c0b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fedb814f-7903-410f-acfa-fb64152b6c0b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' create', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-606db572-0d4a-4d6e-a274-ab2bb202bad2', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-606db572-0d4a-4d6e-a274-ab2bb202bad2', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' `', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9dd37b39-ba9e-48e7-9810-ebb4b367743b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9dd37b39-ba9e-48e7-9810-ebb4b367743b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-23629a8d-3f00-4ea5-9a16-84bf6f8373bf', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-23629a8d-3f00-4ea5-9a16-84bf6f8373bf', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-666c4406-21bc-45c2-94ba-7e9c639df269', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-666c4406-21bc-45c2-94ba-7e9c639df269', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'py', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f32767a5-f12a-46b0-b6c1-c8490cbdf8fe', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f32767a5-f12a-46b0-b6c1-c8490cbdf8fe', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '`', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:54 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a100d9a1-f441-4554-8800-e868eb8ecb52', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:54 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a100d9a1-f441-4554-8800-e868eb8ecb52', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' with', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-61d8b67c-4c36-4932-968e-d0cef86518fa', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-61d8b67c-4c36-4932-968e-d0cef86518fa', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-92d5c60c-b4f3-49ba-87e8-1e7cd6d91e44', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-92d5c60c-b4f3-49ba-87e8-1e7cd6d91e44', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' function', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-231ef7f6-0a61-48ba-967d-ee0f44823432', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-231ef7f6-0a61-48ba-967d-ee0f44823432', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-418fc2a3-c3aa-4d3d-a0dc-1313758e5363', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-418fc2a3-c3aa-4d3d-a0dc-1313758e5363', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '()', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fed5dbb9-4720-455d-9cf8-73a58d643e62', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fed5dbb9-4720-455d-9cf8-73a58d643e62', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' that', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-41942f4a-480a-47e3-a06d-e99190e8744e', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' that', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-41942f4a-480a-47e3-a06d-e99190e8744e', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' that', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' prints', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:55 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cc777fd7-ceb1-41c4-bd96-0f56f024891c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' prints', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:55 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cc777fd7-ceb1-41c4-bd96-0f56f024891c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' prints', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3e6ee713-8b10-4c27-8ac8-4c9acdc5aa8b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3e6ee713-8b10-4c27-8ac8-4c9acdc5aa8b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '‚Äò', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-bc87ba98-d149-4431-8e79-f1b54cf44227', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='‚Äò', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-bc87ba98-d149-4431-8e79-f1b54cf44227', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='‚Äò', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f2cebc3a-8fa6-4258-a41a-1c5de88fa55d', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f2cebc3a-8fa6-4258-a41a-1c5de88fa55d', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-46f3b3f6-b92f-4a81-a5f7-19087babb170', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-46f3b3f6-b92f-4a81-a5f7-19087babb170', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-58c70f0a-9c31-4b3f-9431-fc482e7b42b6', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-58c70f0a-9c31-4b3f-9431-fc482e7b42b6', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-30ead630-14b4-4a2e-8895-e04c2dbafefb', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-30ead630-14b4-4a2e-8895-e04c2dbafefb', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4af4b5de-a907-4619-b2f7-0cb0230814be', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4af4b5de-a907-4619-b2f7-0cb0230814be', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:56 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ab756ef8-e607-4f31-ba88-2320ac7e8292', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:56 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ab756ef8-e607-4f31-ba88-2320ac7e8292', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '‚Äô', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5e832386-77ad-4b86-8617-1bd527afdc92', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='‚Äô', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5e832386-77ad-4b86-8617-1bd527afdc92', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='‚Äô', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ':', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-93b05e9c-a867-4541-8cfa-54930e173c4c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=':', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-93b05e9c-a867-4541-8cfa-54930e173c4c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=':', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-98ec38c0-c282-4284-927d-6c735185acd9', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-98ec38c0-c282-4284-927d-6c735185acd9', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b1c433f1-422b-4a41-9c91-b592beef4584', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b1c433f1-422b-4a41-9c91-b592beef4584', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3114dde0-5aa7-4d46-95b5-875646a8f987', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3114dde0-5aa7-4d46-95b5-875646a8f987', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'python', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:57 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b8f39054-0c7a-4c21-9cda-feeadf8dfcb5', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:57 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b8f39054-0c7a-4c21-9cda-feeadf8dfcb5', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-68d83a9b-fc84-41f5-9f35-49dc752f8960', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-68d83a9b-fc84-41f5-9f35-49dc752f8960', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c44f313b-d31b-4af4-9735-8332401bf70c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c44f313b-d31b-4af4-9735-8332401bf70c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1ec28dad-cd0a-4aa6-b70f-5eb42d9517db', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1ec28dad-cd0a-4aa6-b70f-5eb42d9517db', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9f9c5762-3152-48a4-a2e9-e176e44c54df', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9f9c5762-3152-48a4-a2e9-e176e44c54df', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7af341c6-a95f-4743-9205-6431c66a3609', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7af341c6-a95f-4743-9205-6431c66a3609', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' SE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-50ead158-6c6b-41b7-8215-34116280c391', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-50ead158-6c6b-41b7-8215-34116280c391', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ARCH', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:58 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4f1d3842-2543-48b0-9626-9de4a7a57c2c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:58 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4f1d3842-2543-48b0-9626-9de4a7a57c2c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b38f7043-a7ba-4d88-bfb7-b4fc83caab78', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b38f7043-a7ba-4d88-bfb7-b4fc83caab78', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '======', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f41f5dbc-2cbb-4faa-8a93-0982c551cc2e', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f41f5dbc-2cbb-4faa-8a93-0982c551cc2e', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '=', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-44fe6a4b-e2e2-431e-b0a1-2a6afc3f9fa5', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-44fe6a4b-e2e2-431e-b0a1-2a6afc3f9fa5', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-04c4e8dc-0589-4e77-b70d-02a3220b7617', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-04c4e8dc-0589-4e77-b70d-02a3220b7617', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'def', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:47:59 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5222dde2-e91d-4394-af64-0aeb8d4955e9', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:47:59 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5222dde2-e91d-4394-af64-0aeb8d4955e9', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6df2eaab-1ba3-423d-b1f4-2f505228ea2a', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6df2eaab-1ba3-423d-b1f4-2f505228ea2a', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '():', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a20d5a1c-4739-41a0-9f93-bd3620c31173', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a20d5a1c-4739-41a0-9f93-bd3620c31173', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1e1daaf8-ed62-47f9-abe0-0b8c39dfd000', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1e1daaf8-ed62-47f9-abe0-0b8c39dfd000', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '    ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f1ce77f6-6c3a-4523-93ec-5a35ca749814', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='    ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f1ce77f6-6c3a-4523-93ec-5a35ca749814', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='    ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' "', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-76e925e0-b022-48e5-9d5b-b5c8347a2782', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-76e925e0-b022-48e5-9d5b-b5c8347a2782', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:00 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-800dda64-db57-46be-bc7e-a137a89e54b6', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:00 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-800dda64-db57-46be-bc7e-a137a89e54b6', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9bd81581-1509-423d-bfb0-ab3dd2672fc7', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9bd81581-1509-423d-bfb0-ab3dd2672fc7', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-80bbf45c-19f5-4c0b-8215-3594d775c609', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-80bbf45c-19f5-4c0b-8215-3594d775c609', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ing', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fb37e9be-5faf-4dbd-84dc-126866cd4098', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ing', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fb37e9be-5faf-4dbd-84dc-126866cd4098', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ing', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '"', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-386a4b71-c894-49c0-861f-f777d9172216', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-386a4b71-c894-49c0-861f-f777d9172216', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0f8ce81e-94d4-4164-a10a-4ae84fbb4084', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0f8ce81e-94d4-4164-a10a-4ae84fbb4084', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:01 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9b7b7e6c-21b8-46e9-b3f0-c79734f6e2a2', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:01 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9b7b7e6c-21b8-46e9-b3f0-c79734f6e2a2', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '   ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b2dbbdc8-4367-47ba-9415-78163bbfe056', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b2dbbdc8-4367-47ba-9415-78163bbfe056', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-03655888-e72e-4cd3-94d5-baf4b3eeb395', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-03655888-e72e-4cd3-94d5-baf4b3eeb395', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '("', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-72355eb0-8d30-4a78-babf-4c440485a4c1', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-72355eb0-8d30-4a78-babf-4c440485a4c1', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-435fb83d-19c3-4531-8bac-4fc95520301d', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-435fb83d-19c3-4531-8bac-4fc95520301d', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-232c6383-b17b-4475-a465-2a36887716d5', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-232c6383-b17b-4475-a465-2a36887716d5', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b15f2201-e13c-418d-a94b-9d5d7381ed59', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b15f2201-e13c-418d-a94b-9d5d7381ed59', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:02 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-89786beb-2ddf-4561-8956-d7efbb1511ba', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:02 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-89786beb-2ddf-4561-8956-d7efbb1511ba', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f3d1518a-5cca-40a7-9528-c1c7b6f7e551', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f3d1518a-5cca-40a7-9528-c1c7b6f7e551', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-75fb90ee-cc03-41b4-9081-46a427998b65', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-75fb90ee-cc03-41b4-9081-46a427998b65', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '")', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7377743f-66f6-487f-8eda-bffbca4aed5b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7377743f-66f6-487f-8eda-bffbca4aed5b', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fcb089e5-e6cc-4a50-a737-7382b5bd0cc3', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fcb089e5-e6cc-4a50-a737-7382b5bd0cc3', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f08c5881-1da0-4751-a50d-9fa70054f3ac', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f08c5881-1da0-4751-a50d-9fa70054f3ac', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:03 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-878b6937-6a19-493a-a6b9-d7be8aea2474', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:03 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-878b6937-6a19-493a-a6b9-d7be8aea2474', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f731efbe-e6fe-459a-8822-13a7e0fe2a49', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f731efbe-e6fe-459a-8822-13a7e0fe2a49', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' RE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a51df82e-9f9b-4243-9bfa-cf6411a85cfa', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a51df82e-9f9b-4243-9bfa-cf6411a85cfa', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'PL', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2e2558e6-bea2-4a25-b8a9-495bd94b9703', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2e2558e6-bea2-4a25-b8a9-495bd94b9703', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ACE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-081633df-20ff-4a28-a595-6c8c96d7508c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-081633df-20ff-4a28-a595-6c8c96d7508c', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-246889c4-ae2e-473d-b0b7-83d33550f9b4', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-246889c4-ae2e-473d-b0b7-83d33550f9b4', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3e8805ed-eb64-4a08-bfb6-975b84e8eb61', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3e8805ed-eb64-4a08-bfb6-975b84e8eb61', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:04 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cd98ab82-9ed1-4d59-9340-214c23927968', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:04 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cd98ab82-9ed1-4d59-9340-214c23927968', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:05 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '', 'is_finished': True, 'finish_reason': 'stop', 'usage': {'prompt_tokens': 3303, 'completion_tokens': 80, 'total_tokens': 3383}}
[92m14:48:05 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-267f61ea-283a-4830-a7bf-cf9c39fbc636', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=80, prompt_tokens=3303, total_tokens=3383, completion_tokens_details=None, prompt_tokens_details=None))
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-267f61ea-283a-4830-a7bf-cf9c39fbc636', created=1770562073, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:349 - Logging Details LiteLLM-Async Success Call, cache_hit=False
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:349 - Async success callbacks: Got a complete streaming response
[92m14:48:05 - LiteLLM:DEBUG[0m: litellm_logging.py:1603 - Logging Details LiteLLM-Success Call: Cache_hit=False
[92m14:48:05 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:48:05 - LiteLLM:DEBUG[0m: litellm_logging.py:1632 - Logging Details LiteLLM-Success Call streaming complete
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:05 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:48:05 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:48:05 - LiteLLM:DEBUG[0m: litellm_logging.py:2172 - Model=deepseek-coder:6.7b; cost=0.0
[92m14:48:05 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.sync_deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:151 - in RouterBudgetLimiting.async_log_success_event
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:167 - Not running _PROXY_VirtualKeyModelMaxBudgetLimiter.async_log_success_event because user_api_key_model_max_budget is None or empty. `user_api_key_model_max_budget`={}
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - INSIDE parallel request limiter ASYNC SUCCESS LOGGING
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 3383, 'current_rpm': 0}, precise_minute: 2026-02-08-14-48
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 3383, 'current_rpm': 0}, precise_minute: 2026-02-08-14-48
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:05 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: common_request_processing.py:388 - Request received by LiteLLM:
{
    "messages": [
        {
            "role": "system",
            "content": "Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven't been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user's project.\n\nUse the appropriate shell based on the user's system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        },
        {
            "role": "user",
            "content": "Change get_factorial() to use math.factorial"
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "Refactor hello() into its own file."
        },
        {
            "role": "assistant",
            "content": "To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n"
        },
        {
            "role": "user",
            "content": "I switched to a new code base. Please don't consider the above files or try to edit them any longer."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "I am not sharing any files that you can edit yet."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "Create hello.py with a function greet() that prints 'Hello from Lodestar'"
        },
        {
            "role": "assistant",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    \"print a greeting\"\n\n    print(\"Hello from Lodestar\")\n>>>>>>> REPLACE\n```"
        },
        {
            "role": "user",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"
        },
        {
            "role": "assistant",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    \"print a greeting\"\n\n    print(\"Hello from Lodestar\")\n>>>>>>> REPLACE\n```"
        },
        {
            "role": "user",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"
        },
        {
            "role": "assistant",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints \u2018Hello from Lodestar\u2019:\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n     \"print a greeting\"\n\n    print(\"Hello from Lodestar\")\n>>>>>>> REPLACE\n```"
        },
        {
            "role": "user",
            "content": "Sure, here's how you can create `hello.py` with a function greet() that prints \u2018Hello from Lodestar\u2019:\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"
        },
        {
            "role": "system",
            "content": "# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n"
        }
    ],
    "model": "gpt-3.5-turbo",
    "stream": true,
    "temperature": 0
}
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:664 - Request Headers: Headers({'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '12175'})
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:670 - receiving data: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n     "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '12175'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n     "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '12175'}}}
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:839 - [PROXY] returned data from litellm_pre_call_utils: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n     "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '12175'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n     "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '12175'}}, 'metadata': {'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '12175'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': ''}}
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:349 - Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510>, <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0>, <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0>]
[92m14:48:05 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {}
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: utils.py:1025 - Inside Proxy Logging Pre-call hook!
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: max_budget_limiter.py:23 - Inside Max Budget Limiter Pre-Call Hook
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - Inside Max Parallel Request Pre-Call Hook
[92m14:48:05 - LiteLLM Proxy:INFO[0m: parallel_request_limiter.py:68 - Current Usage of key in this minute: {'current_requests': 0, 'current_tpm': 3383, 'current_rpm': 0}
[92m14:48:05 - LiteLLM Proxy:DEBUG[0m: cache_control_check.py:27 - Inside Cache Control Check Pre-Call Hook
[92m14:48:05 - LiteLLM Router:DEBUG[0m: router.py:3959 - Inside async function with retries.
[92m14:48:05 - LiteLLM Router:DEBUG[0m: router.py:3979 - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x7f4a04698690>>, num_retries - 3
[92m14:48:05 - LiteLLM Router:DEBUG[0m: router.py:6621 - initial list of deployments: [{'model_name': 'gpt-3.5-turbo', 'litellm_params': {'api_base': 'http://192.168.120.211:11434', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'ollama/deepseek-coder:6.7b'}, 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}}]
[92m14:48:05 - LiteLLM Router:DEBUG[0m: cooldown_handlers.py:342 - retrieve cooldown models: []
[92m14:48:05 - LiteLLM Router:DEBUG[0m: router.py:6688 - async cooldown deployments: []
[92m14:48:05 - LiteLLM Router:DEBUG[0m: router.py:6691 - cooldown_deployments: []
[92m14:48:05 - LiteLLM Router:DEBUG[0m: router.py:7055 - cooldown deployments: []
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:349 - [92mRequest to litellm:[0m
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:349 - [92mlitellm.acompletion(api_base='http://192.168.120.211:11434', use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='ollama/deepseek-coder:6.7b', messages=[{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n     "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], caching=False, client=None, temperature=0, proxy_server_request={'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '12175'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n     "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, secret_fields={'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '12175'}}, metadata={'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '12175'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': '', 'model_group': 'gpt-3.5-turbo', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'ollama/deepseek-coder:6.7b', 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, 'api_base': 'http://192.168.120.211:11434', 'deployment_model_name': 'gpt-3.5-turbo', 'caching_groups': None}, litellm_call_id='6499d1f7-2fef-4884-abaf-c68066e4fc32', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x7f4a00b6c590>, stream=True, litellm_trace_id='b9684e93-bffc-4bfa-a3a6-af4957fe2cef', model_info={'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, timeout=600, max_retries=0)[0m
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:349 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
[92m14:48:05 - LiteLLM:DEBUG[0m: caching_handler.py:213 - CACHE RESULT: None
[92m14:48:05 - LiteLLM:INFO[0m: utils.py:3260 - 
LiteLLM completion() model= deepseek-coder:6.7b; provider = ollama
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:3263 - 
LiteLLM: Params passed to completion() {'model': 'deepseek-coder:6.7b', 'functions': None, 'function_call': None, 'temperature': 0, 'top_p': None, 'n': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}, {'role': 'user', 'content': 'Change get_factorial() to use math.factorial'}, {'role': 'assistant', 'content': 'To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': 'Refactor hello() into its own file.'}, {'role': 'assistant', 'content': 'To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints 'Hello from Lodestar':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'assistant', 'content': 'Sure, here\'s how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n     "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```'}, {'role': 'user', 'content': "Sure, here's how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```"}, {'role': 'system', 'content': '# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n'}], 'thinking': None, 'web_search_options': None}
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:3266 - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:349 - Final returned optional params: {'temperature': 0, 'stream': True}
[92m14:48:05 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:05 - LiteLLM:DEBUG[0m: litellm_logging.py:930 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
http://192.168.120.211:11434/api/generate \
-d '{'model': 'deepseek-coder:6.7b', 'prompt': '### System:\nAct as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\nReply in English.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven\'t been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user\'s project.\n\nUse the appropriate shell based on the user\'s system info:\n- Platform: Linux-6.1.0-42-amd64-x86_64-with-glibc2.36\n- Shell: SHELL=/bin/bash\n- Language: English\n- Current date: 2026-02-08\n- The user is operating inside a git repository\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n### User:\nChange get_factorial() to use math.factorial\n\n### Assistant:\nTo make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\ndef factorial(n):\n    "compute factorial"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\nmathweb/flask/app.py\n```python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n\n### User:\nRefactor hello() into its own file.\n\n### Assistant:\nTo make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n```python\n<<<<<<< SEARCH\n=======\ndef hello():\n    "print a greeting"\n\n    print("hello")\n>>>>>>> REPLACE\n```\n\nmain.py\n```python\n<<<<<<< SEARCH\ndef hello():\n    "print a greeting"\n\n    print("hello")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n\n### User:\nI switched to a new code base. Please don\'t consider the above files or try to edit them any longer.\n\n### Assistant:\nOk.\n\n### User:\nI am not sharing any files that you can edit yet.\n\n### Assistant:\nOk.\n\n### User:\nCreate hello.py with a function greet() that prints \'Hello from Lodestar\'\n\n### Assistant:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```\n\n### User:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```\n\n### Assistant:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n    "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```\n\n### User:\nSure, here\'s how you can create `hello.py` with a function greet() that prints \'Hello from Lodestar\':\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```\n\n### Assistant:\nSure, here\'s how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n=======\ndef greet():\n     "print a greeting"\n\n    print("Hello from Lodestar")\n>>>>>>> REPLACE\n```\n\n### User:\nSure, here\'s how you can create `hello.py` with a function greet() that prints ‚ÄòHello from Lodestar‚Äô:\n\n```python\n<<<<<<< SEARCH\n\n^^^ Bad/missing filename. The filename must be alone on the line before the opening fence ```\n\n### System:\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: ```python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file\'s contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they\'ve applied your edits. If they haven\'t explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\nReply in English.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\n\n\n\n', 'options': {'temperature': 0}, 'stream': True, 'images': []}'
[0m

[92m14:48:24 - LiteLLM:DEBUG[0m: utils.py:349 - RAW RESPONSE:
first stream response received


[92m14:48:24 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:48:24 - LiteLLM:DEBUG[0m: token_counter.py:374 - messages in token_counter: None, text in token_counter: 
[92m14:48:24 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:24 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:24 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:24 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:48:24 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:48:24 - LiteLLM Router:INFO[0m: router.py:1331 - litellm.acompletion(model=ollama/deepseek-coder:6.7b)[32m 200 OK[0m
[92m14:48:24 - LiteLLM Router:DEBUG[0m: router.py:3892 - Async Response: <litellm.router.Router._acompletion_streaming_iterator.<locals>.FallbackStreamWrapper object at 0x7f4a00b6c290>
[92m14:48:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3273 - inside generator
[92m14:48:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Sure', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6cae543f-473e-44c0-a70e-77d338d8c4fe', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6cae543f-473e-44c0-a70e-77d338d8c4fe', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
INFO:     127.0.0.1:47124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[92m14:48:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ',', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-20c98e3d-a2b5-4cdd-8bdf-a1605dd9bf4b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-20c98e3d-a2b5-4cdd-8bdf-a1605dd9bf4b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' here', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-234a8cef-9f41-4705-8c36-c4eec11c18f2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-234a8cef-9f41-4705-8c36-c4eec11c18f2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': "'", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f38b2836-ec51-4ceb-8d28-b5e601277baa', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="'", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f38b2836-ec51-4ceb-8d28-b5e601277baa', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="'", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 's', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0bcf4dd6-4d30-420b-95ed-f5cb7ec7dd0b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='s', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0bcf4dd6-4d30-420b-95ed-f5cb7ec7dd0b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='s', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' how', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3e39479e-3e3d-4e52-84f8-8c459ec7264f', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' how', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3e39479e-3e3d-4e52-84f8-8c459ec7264f', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' how', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' you', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e41c6ad4-dac8-4d55-a148-eee69e646d42', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e41c6ad4-dac8-4d55-a148-eee69e646d42', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' can', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4bba7556-2c30-493e-8406-2449afe4dea1', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4bba7556-2c30-493e-8406-2449afe4dea1', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' create', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6df8eba0-642b-45c1-847e-5c11811286fb', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6df8eba0-642b-45c1-847e-5c11811286fb', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' create', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' `', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:25 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b0b631dc-966c-4cc0-9796-94bf81595a51', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:25 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b0b631dc-966c-4cc0-9796-94bf81595a51', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' `', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-75fd1090-06fd-46aa-bd26-e92bdb0ae068', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-75fd1090-06fd-46aa-bd26-e92bdb0ae068', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-859a2714-c2cb-47ea-b40f-268d11d28bf2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-859a2714-c2cb-47ea-b40f-268d11d28bf2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'py', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-14cadd92-9c93-420d-a81d-94d81be91979', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-14cadd92-9c93-420d-a81d-94d81be91979', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '`', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-011b73f7-bf8d-4454-8e14-71d7cccbbdf3', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-011b73f7-bf8d-4454-8e14-71d7cccbbdf3', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='`', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' with', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-769c254f-02ff-4a51-b1b7-7aee6d26d2af', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-769c254f-02ff-4a51-b1b7-7aee6d26d2af', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' with', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-41a0bdc9-d770-4ddf-971d-3f735ade6f7c', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-41a0bdc9-d770-4ddf-971d-3f735ade6f7c', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' function', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:26 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6122354e-6d31-4c41-b916-97f3d5cfc4c0', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:26 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6122354e-6d31-4c41-b916-97f3d5cfc4c0', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fa12507a-1108-4018-a9bc-9ffd77298a43', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fa12507a-1108-4018-a9bc-9ffd77298a43', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '()', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e7ccb268-0954-440c-81f1-d9cfdf85c9b9', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e7ccb268-0954-440c-81f1-d9cfdf85c9b9', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='()', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' that', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-073b4057-bc0a-4ad8-af6c-cfd5957a1f90', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' that', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-073b4057-bc0a-4ad8-af6c-cfd5957a1f90', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' that', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' prints', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-92676ae7-6a76-4e62-9645-6efb17eb1f49', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' prints', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-92676ae7-6a76-4e62-9645-6efb17eb1f49', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' prints', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-25b3de44-852d-4327-b978-c315c32f9234', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-25b3de44-852d-4327-b978-c315c32f9234', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '‚Äò', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-486fe5e2-1b58-4ec4-bb17-5a4fa73e1b56', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='‚Äò', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-486fe5e2-1b58-4ec4-bb17-5a4fa73e1b56', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='‚Äò', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:27 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5c75f842-63d3-4455-a268-578121b4fd55', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:27 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5c75f842-63d3-4455-a268-578121b4fd55', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f1488924-56f6-4fba-9a20-e8177109edf7', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f1488924-56f6-4fba-9a20-e8177109edf7', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1492f258-bd0c-46c9-80e8-5dbde2f8c9b0', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1492f258-bd0c-46c9-80e8-5dbde2f8c9b0', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-79575a0f-830c-4517-9d77-b0f6b76f7676', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-79575a0f-830c-4517-9d77-b0f6b76f7676', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f26aefda-a90f-422b-8450-85376bc8a577', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f26aefda-a90f-422b-8450-85376bc8a577', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7517573f-8136-40a1-9c58-c3cd5cf03256', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7517573f-8136-40a1-9c58-c3cd5cf03256', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '‚Äô', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:28 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-231a12c7-12a7-4ebf-afe6-d6c7f80d1350', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='‚Äô', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:28 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-231a12c7-12a7-4ebf-afe6-d6c7f80d1350', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='‚Äô', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ':', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b236b36d-92b7-43af-ba3a-1113e12ab8a4', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=':', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b236b36d-92b7-43af-ba3a-1113e12ab8a4', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=':', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-85dd2c0c-a8fe-4f89-8c4d-386199ec164b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-85dd2c0c-a8fe-4f89-8c4d-386199ec164b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-810fd05d-2d9c-4a9b-ae9e-673beaa67424', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-810fd05d-2d9c-4a9b-ae9e-673beaa67424', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-079d33bc-cc3b-4dd7-9109-b8cbdd4a113d', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-079d33bc-cc3b-4dd7-9109-b8cbdd4a113d', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'python', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-61f544a9-99f7-4a15-b7eb-b721dcb56297', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-61f544a9-99f7-4a15-b7eb-b721dcb56297', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='python', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:29 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-c175c7f8-22c1-456e-b27b-26d2ed19b7db', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:29 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-c175c7f8-22c1-456e-b27b-26d2ed19b7db', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cd322558-3263-4187-a50d-696722740456', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cd322558-3263-4187-a50d-696722740456', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-91fb7e6d-6dd4-4a40-86d8-175585b1661a', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-91fb7e6d-6dd4-4a40-86d8-175585b1661a', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:30 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-eff6f2cb-5b44-498d-bcea-845512042d87', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:30 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-eff6f2cb-5b44-498d-bcea-845512042d87', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '<', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:31 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9fbee9f9-179e-4b8c-8f43-c2911c2ec629', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:31 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9fbee9f9-179e-4b8c-8f43-c2911c2ec629', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='<', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' SE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cfab6c46-465a-42d0-a2dc-ae8224dd5b9e', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cfab6c46-465a-42d0-a2dc-ae8224dd5b9e', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' SE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ARCH', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-e380d063-c606-4c97-a798-42757b1b7983', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-e380d063-c606-4c97-a798-42757b1b7983', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ARCH', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-00a00d91-3f8d-4173-b0dd-e0171bc88a9a', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-00a00d91-3f8d-4173-b0dd-e0171bc88a9a', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '======', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ad7e0b9c-0280-40e6-999d-e1e95002c26e', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ad7e0b9c-0280-40e6-999d-e1e95002c26e', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='======', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '=', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-daab43ae-7636-44c1-b6eb-6c29708b220d', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-daab43ae-7636-44c1-b6eb-6c29708b220d', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='=', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-af7cd1c7-e9a9-4b04-9be3-c73b014ada53', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-af7cd1c7-e9a9-4b04-9be3-c73b014ada53', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'def', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:32 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-77704e7a-3fcf-4dca-b842-1bd7a862a8ce', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:32 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-77704e7a-3fcf-4dca-b842-1bd7a862a8ce', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-a0230149-6129-4d48-b583-42ddafc32cbd', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-a0230149-6129-4d48-b583-42ddafc32cbd', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '():', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b507c62d-23da-4c8e-8313-ec814f8bb766', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b507c62d-23da-4c8e-8313-ec814f8bb766', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-618eb589-df65-45c2-8394-b272a045849d', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-618eb589-df65-45c2-8394-b272a045849d', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '    ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-094e4cd6-f6b3-4ca6-ac4d-4c5f5b0ec19b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='    ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-094e4cd6-f6b3-4ca6-ac4d-4c5f5b0ec19b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='    ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' "', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-831cce54-667a-44a2-a947-d808376d7dd1', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-831cce54-667a-44a2-a947-d808376d7dd1', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' "', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9c4cd4d2-0d3c-4edf-94bd-c9a08599657a', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9c4cd4d2-0d3c-4edf-94bd-c9a08599657a', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' a', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:33 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b2a6969a-1490-4a01-8a0b-60cf948c6d61', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:33 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b2a6969a-1490-4a01-8a0b-60cf948c6d61', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' a', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3c8a0eaa-4e8b-4d0d-a51d-fb41cd0cf2b5', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3c8a0eaa-4e8b-4d0d-a51d-fb41cd0cf2b5', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ing', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7f58b907-a48e-47ea-8c39-974895353ea2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ing', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7f58b907-a48e-47ea-8c39-974895353ea2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ing', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '"', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f2c0d067-f010-4e77-a526-eada9695357b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f2c0d067-f010-4e77-a526-eada9695357b', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='"', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-933b5e03-8609-4b11-bafa-4da926a13d4f', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-933b5e03-8609-4b11-bafa-4da926a13d4f', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cbd10dd8-1543-43e7-a2c3-c055d2e011ca', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cbd10dd8-1543-43e7-a2c3-c055d2e011ca', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '   ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1eaa9fad-ec0c-42d3-9506-c7943c3d84c9', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1eaa9fad-ec0c-42d3-9506-c7943c3d84c9', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:34 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b262a496-6b05-4186-b699-6c16bde2d42e', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:34 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b262a496-6b05-4186-b699-6c16bde2d42e', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '("', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-afbbecb5-bf1a-4d75-8278-30140af53b63', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-afbbecb5-bf1a-4d75-8278-30140af53b63', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='("', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-60886165-60d3-45fd-a40e-a42695abe7d8', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-60886165-60d3-45fd-a40e-a42695abe7d8', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-db02b4dd-bffa-437b-876b-c90b56400b57', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-db02b4dd-bffa-437b-876b-c90b56400b57', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7920a870-5bbd-412e-be2f-916bad8c9719', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7920a870-5bbd-412e-be2f-916bad8c9719', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-53bbf696-a75d-47a5-b384-ddd893312bb7', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-53bbf696-a75d-47a5-b384-ddd893312bb7', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:35 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cfedf8b5-8a55-4763-8b7f-79f73c7406b2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:35 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cfedf8b5-8a55-4763-8b7f-79f73c7406b2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-f1b2b3bf-e17c-4cc7-b4a9-638483fbc556', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-f1b2b3bf-e17c-4cc7-b4a9-638483fbc556', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '")', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9c539f9b-ea09-425a-a7bd-3007ace07c92', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9c539f9b-ea09-425a-a7bd-3007ace07c92', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='")', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-93e86062-8e3d-4f47-afd4-196424c38c32', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-93e86062-8e3d-4f47-afd4-196424c38c32', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7a043876-fc76-46de-8bfb-dcf997d4f348', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7a043876-fc76-46de-8bfb-dcf997d4f348', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-588234a0-0057-4d69-b514-dbb0ff337e2d', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-588234a0-0057-4d69-b514-dbb0ff337e2d', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '>>>', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5b1efa92-9092-401a-bcc9-65539d9c9112', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5b1efa92-9092-401a-bcc9-65539d9c9112', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='>>>', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' RE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:36 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-6d54675d-6628-4bb0-8e5d-a336a0cbf1be', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:36 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-6d54675d-6628-4bb0-8e5d-a336a0cbf1be', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' RE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'PL', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d8ce8358-9c78-479b-84fe-1e9470a3a7f1', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d8ce8358-9c78-479b-84fe-1e9470a3a7f1', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='PL', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ACE', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1357c60c-b10a-4aaa-9325-d1b56ed4e6f2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1357c60c-b10a-4aaa-9325-d1b56ed4e6f2', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ACE', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-cecfbcc8-b597-4dc2-9ca5-3f4fd4e18b49', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-cecfbcc8-b597-4dc2-9ca5-3f4fd4e18b49', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ef7db0dc-0865-47f8-bed7-ce9ae8a3c938', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ef7db0dc-0865-47f8-bed7-ce9ae8a3c938', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-71d5c943-92cb-484a-bd40-bc629f2f7a2c', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-71d5c943-92cb-484a-bd40-bc629f2f7a2c', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '', 'is_finished': True, 'finish_reason': 'stop', 'usage': {'prompt_tokens': 3459, 'completion_tokens': 80, 'total_tokens': 3539}}
[92m14:48:37 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-538a8223-ca69-4834-8294-357d8e013c93', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=80, prompt_tokens=3459, total_tokens=3539, completion_tokens_details=None, prompt_tokens_details=None))
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-538a8223-ca69-4834-8294-357d8e013c93', created=1770562104, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:349 - Logging Details LiteLLM-Async Success Call, cache_hit=False
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:349 - Async success callbacks: Got a complete streaming response
[92m14:48:37 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:37 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:48:37 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:48:37 - LiteLLM:DEBUG[0m: litellm_logging.py:2172 - Model=deepseek-coder:6.7b; cost=0.0
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:37 - LiteLLM:DEBUG[0m: litellm_logging.py:1603 - Logging Details LiteLLM-Success Call: Cache_hit=False
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:48:37 - LiteLLM:DEBUG[0m: litellm_logging.py:1632 - Logging Details LiteLLM-Success Call streaming complete
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:151 - in RouterBudgetLimiting.async_log_success_event
[92m14:48:37 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:167 - Not running _PROXY_VirtualKeyModelMaxBudgetLimiter.async_log_success_event because user_api_key_model_max_budget is None or empty. `user_api_key_model_max_budget`={}
[92m14:48:37 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - INSIDE parallel request limiter ASYNC SUCCESS LOGGING
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 6922, 'current_rpm': 1}, precise_minute: 2026-02-08-14-48
[92m14:48:37 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 6922, 'current_rpm': 0}, precise_minute: 2026-02-08-14-48
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:48:37 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.sync_deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:48:37 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:49:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m14:49:59 - LiteLLM Proxy:DEBUG[0m: model_checks.py:121 - ALL KEY MODELS - 0
[92m14:49:59 - LiteLLM Proxy:DEBUG[0m: model_checks.py:153 - ALL TEAM MODELS - 0
INFO:     127.0.0.1:57460 - "GET /v1/models HTTP/1.1" 200 OK
[92m14:49:59 - LiteLLM Proxy:DEBUG[0m: model_checks.py:121 - ALL KEY MODELS - 0
[92m14:49:59 - LiteLLM Proxy:DEBUG[0m: model_checks.py:153 - ALL TEAM MODELS - 0
INFO:     127.0.0.1:57464 - "GET /v1/models HTTP/1.1" 200 OK
[92m14:50:08 - LiteLLM Proxy:DEBUG[0m: common_request_processing.py:388 - Request received by LiteLLM:
{
    "messages": [
        {
            "role": "system",
            "content": "Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nReply in English.\n\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using \"...\" or by adding comments like \"... rest of code...\"!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n"
        },
        {
            "role": "user",
            "content": "Change the greeting to be more casual"
        },
        {
            "role": "assistant",
            "content": "Ok, I will:\n\n1. Switch the greeting text from \"Hello\" to \"Hey\".\n\nshow_greeting.py\n```\nimport sys\n\ndef greeting(name):\n    print(f\"Hey {name}\")\n\nif __name__ == '__main__':\n    greeting(sys.argv[1])\n```\n"
        },
        {
            "role": "user",
            "content": "I switched to a new code base. Please don't consider the above files or try to edit them any longer."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "I am not sharing any files that you can edit yet."
        },
        {
            "role": "assistant",
            "content": "Ok."
        },
        {
            "role": "user",
            "content": "Create hello.py with a function greet() that prints 'Hello from Lodestar'"
        },
        {
            "role": "system",
            "content": "To suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using \"...\" or by adding comments like \"... rest of code...\"!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n"
        }
    ],
    "model": "gpt-3.5-turbo",
    "stream": true,
    "temperature": 0
}
[92m14:50:08 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:664 - Request Headers: Headers({'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '2978'})
[92m14:50:08 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:670 - receiving data: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nReply in English.\n\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}, {'role': 'user', 'content': 'Change the greeting to be more casual'}, {'role': 'assistant', 'content': 'Ok, I will:\n\n1. Switch the greeting text from "Hello" to "Hey".\n\nshow_greeting.py\n```\nimport sys\n\ndef greeting(name):\n    print(f"Hey {name}")\n\nif __name__ == \'__main__\':\n    greeting(sys.argv[1])\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': 'To suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '2978'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nReply in English.\n\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}, {'role': 'user', 'content': 'Change the greeting to be more casual'}, {'role': 'assistant', 'content': 'Ok, I will:\n\n1. Switch the greeting text from "Hello" to "Hey".\n\nshow_greeting.py\n```\nimport sys\n\ndef greeting(name):\n    print(f"Hey {name}")\n\nif __name__ == \'__main__\':\n    greeting(sys.argv[1])\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': 'To suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '2978'}}}
[92m14:50:08 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:839 - [PROXY] returned data from litellm_pre_call_utils: {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nReply in English.\n\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}, {'role': 'user', 'content': 'Change the greeting to be more casual'}, {'role': 'assistant', 'content': 'Ok, I will:\n\n1. Switch the greeting text from "Hello" to "Hey".\n\nshow_greeting.py\n```\nimport sys\n\ndef greeting(name):\n    print(f"Hey {name}")\n\nif __name__ == \'__main__\':\n    greeting(sys.argv[1])\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': 'To suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '2978'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nReply in English.\n\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}, {'role': 'user', 'content': 'Change the greeting to be more casual'}, {'role': 'assistant', 'content': 'Ok, I will:\n\n1. Switch the greeting text from "Hello" to "Hey".\n\nshow_greeting.py\n```\nimport sys\n\ndef greeting(name):\n    print(f"Hey {name}")\n\nif __name__ == \'__main__\':\n    greeting(sys.argv[1])\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': 'To suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '2978'}}, 'metadata': {'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '2978'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': ''}}
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:349 - Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510>, <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0>, <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0>]
[92m14:50:08 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {}
[92m14:50:08 - LiteLLM Proxy:DEBUG[0m: utils.py:1025 - Inside Proxy Logging Pre-call hook!
[92m14:50:08 - LiteLLM Proxy:DEBUG[0m: max_budget_limiter.py:23 - Inside Max Budget Limiter Pre-Call Hook
[92m14:50:08 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - Inside Max Parallel Request Pre-Call Hook
[92m14:50:08 - LiteLLM Proxy:INFO[0m: parallel_request_limiter.py:68 - Current Usage of key in this minute: None
[92m14:50:08 - LiteLLM Proxy:DEBUG[0m: cache_control_check.py:27 - Inside Cache Control Check Pre-Call Hook
[92m14:50:08 - LiteLLM Router:DEBUG[0m: router.py:3959 - Inside async function with retries.
[92m14:50:08 - LiteLLM Router:DEBUG[0m: router.py:3979 - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x7f4a04698690>>, num_retries - 3
[92m14:50:08 - LiteLLM Router:DEBUG[0m: router.py:6621 - initial list of deployments: [{'model_name': 'gpt-3.5-turbo', 'litellm_params': {'api_base': 'http://192.168.120.211:11434', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'ollama/deepseek-coder:6.7b'}, 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}}]
[92m14:50:08 - LiteLLM Router:DEBUG[0m: cooldown_handlers.py:342 - retrieve cooldown models: []
[92m14:50:08 - LiteLLM Router:DEBUG[0m: router.py:6688 - async cooldown deployments: []
[92m14:50:08 - LiteLLM Router:DEBUG[0m: router.py:6691 - cooldown_deployments: []
[92m14:50:08 - LiteLLM Router:DEBUG[0m: router.py:7055 - cooldown deployments: []
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:349 - [92mRequest to litellm:[0m
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:349 - [92mlitellm.acompletion(api_base='http://192.168.120.211:11434', use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='ollama/deepseek-coder:6.7b', messages=[{'role': 'system', 'content': 'Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nReply in English.\n\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}, {'role': 'user', 'content': 'Change the greeting to be more casual'}, {'role': 'assistant', 'content': 'Ok, I will:\n\n1. Switch the greeting text from "Hello" to "Hey".\n\nshow_greeting.py\n```\nimport sys\n\ndef greeting(name):\n    print(f"Hey {name}")\n\nif __name__ == \'__main__\':\n    greeting(sys.argv[1])\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': 'To suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}], caching=False, client=None, temperature=0, proxy_server_request={'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '2978'}, 'body': {'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nReply in English.\n\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}, {'role': 'user', 'content': 'Change the greeting to be more casual'}, {'role': 'assistant', 'content': 'Ok, I will:\n\n1. Switch the greeting text from "Hello" to "Hey".\n\nshow_greeting.py\n```\nimport sys\n\ndef greeting(name):\n    print(f"Hey {name}")\n\nif __name__ == \'__main__\':\n    greeting(sys.argv[1])\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': 'To suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}], 'model': 'gpt-3.5-turbo', 'stream': True, 'temperature': 0}}, secret_fields={'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '2978'}}, metadata={'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '2978'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': '', 'model_group': 'gpt-3.5-turbo', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'ollama/deepseek-coder:6.7b', 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, 'api_base': 'http://192.168.120.211:11434', 'deployment_model_name': 'gpt-3.5-turbo', 'caching_groups': None}, litellm_call_id='9bcf2d2d-ad98-4305-a6fb-fa8e7b7e8771', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x7f4a00b72750>, stream=True, litellm_trace_id='54943828-08a5-4da1-87c8-e6efa26ee52c', model_info={'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, timeout=600, max_retries=0)[0m
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:349 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
[92m14:50:08 - LiteLLM:DEBUG[0m: caching_handler.py:213 - CACHE RESULT: None
[92m14:50:08 - LiteLLM:INFO[0m: utils.py:3260 - 
LiteLLM completion() model= deepseek-coder:6.7b; provider = ollama
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:3263 - 
LiteLLM: Params passed to completion() {'model': 'deepseek-coder:6.7b', 'functions': None, 'function_call': None, 'temperature': 0, 'top_p': None, 'n': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nReply in English.\n\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}, {'role': 'user', 'content': 'Change the greeting to be more casual'}, {'role': 'assistant', 'content': 'Ok, I will:\n\n1. Switch the greeting text from "Hello" to "Hey".\n\nshow_greeting.py\n```\nimport sys\n\ndef greeting(name):\n    print(f"Hey {name}")\n\nif __name__ == \'__main__\':\n    greeting(sys.argv[1])\n```\n'}, {'role': 'user', 'content': "I switched to a new code base. Please don't consider the above files or try to edit them any longer."}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': 'I am not sharing any files that you can edit yet.'}, {'role': 'assistant', 'content': 'Ok.'}, {'role': 'user', 'content': "Create hello.py with a function greet() that prints 'Hello from Lodestar'"}, {'role': 'system', 'content': 'To suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n'}], 'thinking': None, 'web_search_options': None}
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:3266 - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:349 - Final returned optional params: {'temperature': 0, 'stream': True}
[92m14:50:08 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {'temperature': 0, 'stream': True, 'max_retries': 0}
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:08 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:08 - LiteLLM:DEBUG[0m: litellm_logging.py:930 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
http://192.168.120.211:11434/api/generate \
-d '{'model': 'deepseek-coder:6.7b', 'prompt': '### System:\nAct as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in English.\n\nReply in English.\n\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n\n\n### User:\nChange the greeting to be more casual\n\n### Assistant:\nOk, I will:\n\n1. Switch the greeting text from "Hello" to "Hey".\n\nshow_greeting.py\n```\nimport sys\n\ndef greeting(name):\n    print(f"Hey {name}")\n\nif __name__ == \'__main__\':\n    greeting(sys.argv[1])\n```\n\n\n### User:\nI switched to a new code base. Please don\'t consider the above files or try to edit them any longer.\n\n### Assistant:\nOk.\n\n### User:\nI am not sharing any files that you can edit yet.\n\n### Assistant:\nOk.\n\n### User:\nCreate hello.py with a function greet() that prints \'Hello from Lodestar\'\n\n### System:\nTo suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n```\n// entire file content ...\n// ... goes in between\n```\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening ```\n- ... entire content of the file ...\n- Final line: closing ```\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using "..." or by adding comments like "... rest of code..."!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\nReply in English.\n\n\n\n', 'options': {'temperature': 0}, 'stream': True, 'images': []}'
[0m

[92m14:50:20 - LiteLLM:DEBUG[0m: utils.py:349 - RAW RESPONSE:
first stream response received


[92m14:50:20 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:50:20 - LiteLLM:DEBUG[0m: token_counter.py:374 - messages in token_counter: None, text in token_counter: 
[92m14:50:20 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:20 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:20 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:20 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:50:20 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:50:20 - LiteLLM Router:INFO[0m: router.py:1331 - litellm.acompletion(model=ollama/deepseek-coder:6.7b)[32m 200 OK[0m
[92m14:50:20 - LiteLLM Router:DEBUG[0m: router.py:3892 - Async Response: <litellm.router.Router._acompletion_streaming_iterator.<locals>.FallbackStreamWrapper object at 0x7f4a00b72710>
[92m14:50:20 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3273 - inside generator
[92m14:50:20 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Sure', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:20 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1c32aa78-d9ef-481f-ba6d-6924000eba82', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:20 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1c32aa78-d9ef-481f-ba6d-6924000eba82', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Sure', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
INFO:     127.0.0.1:35696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[92m14:50:20 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ',', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:20 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-b2ac967e-baaf-4170-94f7-446ed0341364', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:20 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-b2ac967e-baaf-4170-94f7-446ed0341364', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=',', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' here', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-1cec0fe8-7f14-43d6-9e92-0af626cecebd', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-1cec0fe8-7f14-43d6-9e92-0af626cecebd', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' here', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' is', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-61e42bef-112d-415a-a46a-3458b8280d07', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' is', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-61e42bef-112d-415a-a46a-3458b8280d07', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' is', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' the', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2852fdc2-505d-4b3b-b14f-14f302595e42', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' the', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2852fdc2-505d-4b3b-b14f-14f302595e42', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' the', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' content', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-21de6c2d-d286-45e1-b5ba-184d814aeec0', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' content', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-21de6c2d-d286-45e1-b5ba-184d814aeec0', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' content', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' for', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ba8f3d96-aaf0-4c80-9d12-ca15c8ccb5c4', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' for', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ba8f3d96-aaf0-4c80-9d12-ca15c8ccb5c4', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' for', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' hell', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-ca3348d1-6f7a-46b1-80ad-b3c16cb99eb6', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' hell', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-ca3348d1-6f7a-46b1-80ad-b3c16cb99eb6', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' hell', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'o', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9cd683d7-36b8-43c1-8ebe-28729dedb57f', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='o', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9cd683d7-36b8-43c1-8ebe-28729dedb57f', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='o', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-62decaa5-9f40-4151-b89a-e7be7dc22c20', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-62decaa5-9f40-4151-b89a-e7be7dc22c20', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'py', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-75413fb7-67dd-4a45-87e4-2c5edc70fad9', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-75413fb7-67dd-4a45-87e4-2c5edc70fad9', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ':', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:21 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8078f316-49f6-4de1-b787-6b72fe515a20', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=':', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:21 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8078f316-49f6-4de1-b787-6b72fe515a20', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=':', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-db3b1798-e204-4ffa-8152-3d5777baa7d9', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-db3b1798-e204-4ffa-8152-3d5777baa7d9', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-2e66499b-7181-4e46-b093-a9954b1730e9', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-2e66499b-7181-4e46-b093-a9954b1730e9', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0eaaf442-1b0b-41da-bcb0-3c4e1a655f84', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0eaaf442-1b0b-41da-bcb0-3c4e1a655f84', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '.', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-dd2425ae-65dc-4ac5-9209-06c031a452ce', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-dd2425ae-65dc-4ac5-9209-06c031a452ce', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'py', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8d500edd-d298-48fa-9f62-254423649e0d', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8d500edd-d298-48fa-9f62-254423649e0d', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='py', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3bb28888-7cfc-4d15-873a-9f0c8cd0c846', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3bb28888-7cfc-4d15-873a-9f0c8cd0c846', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-54909e14-c51c-4ce8-a74d-019863f62400', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-54909e14-c51c-4ce8-a74d-019863f62400', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7feae7cc-c87d-4acf-aec0-4576cc06b1ce', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7feae7cc-c87d-4acf-aec0-4576cc06b1ce', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'def', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d88556ae-7c3e-4cba-902f-d48b46a24135', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d88556ae-7c3e-4cba-902f-d48b46a24135', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='def', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' greet', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:22 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-4cef6cfb-6b2b-4ac4-8f3f-3633b8758057', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:22 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-4cef6cfb-6b2b-4ac4-8f3f-3633b8758057', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' greet', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '():', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-dcebc5ae-28d4-4375-a2ff-12f211e2fe29', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-dcebc5ae-28d4-4375-a2ff-12f211e2fe29', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='():', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-7f854188-c1e9-461e-8a78-dbda843dc1ff', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-7f854188-c1e9-461e-8a78-dbda843dc1ff', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '   ', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-97af181a-5e1e-4938-8678-a4c690bdf622', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-97af181a-5e1e-4938-8678-a4c690bdf622', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='   ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' print', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-0809282a-f0f9-4a14-b720-36650407d94f', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-0809282a-f0f9-4a14-b720-36650407d94f', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' print', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': "('", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-67c6d793-723b-436c-b920-d454fa644533', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="('", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-67c6d793-723b-436c-b920-d454fa644533', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="('", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'Hello', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-5eb1f4ea-3d03-4e3f-a086-c93835f20f15', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-5eb1f4ea-3d03-4e3f-a086-c93835f20f15', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Hello', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' from', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-3e6e78e6-0b5b-467c-84f5-425ebbf9c71a', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-3e6e78e6-0b5b-467c-84f5-425ebbf9c71a', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' from', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': ' L', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9b625b42-4d17-46c1-ab0b-00f2a6a04332', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9b625b42-4d17-46c1-ab0b-00f2a6a04332', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' L', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'od', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:23 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-dc35ce63-4724-496c-a36d-d45ce9a3c0b3', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:23 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-dc35ce63-4724-496c-a36d-d45ce9a3c0b3', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='od', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'est', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-d7abbf06-46f2-4d74-89d0-2a9d870b49af', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-d7abbf06-46f2-4d74-89d0-2a9d870b49af', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='est', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': 'ar', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-88814859-cc4f-4e34-851e-f53752d1a350', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-88814859-cc4f-4e34-851e-f53752d1a350', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='ar', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': "')", 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fc23bd5a-4d67-4a8e-81aa-96798f902011', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="')", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fc23bd5a-4d67-4a8e-81aa-96798f902011', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content="')", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-8fcac97e-f78c-4b76-9202-ad4107fd1bdb', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-8fcac97e-f78c-4b76-9202-ad4107fd1bdb', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '```', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-9214c30a-7dd1-4a4d-9ac1-055d85df7e36', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-9214c30a-7dd1-4a4d-9ac1-055d85df7e36', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='```', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '\n', 'is_finished': False, 'finish_reason': 'stop', 'usage': None}
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-fb0b635c-f540-4986-8bff-29ca31b6c10d', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-fb0b635c-f540-4986-8bff-29ca31b6c10d', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='\n', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1692 - PROCESSED ASYNC CHUNK PRE CHUNK CREATOR: {'text': '', 'is_finished': True, 'finish_reason': 'stop', 'usage': {'prompt_tokens': 842, 'completion_tokens': 38, 'total_tokens': 880}}
[92m14:50:24 - LiteLLM:DEBUG[0m: streaming_handler.py:1699 - PROCESSED ASYNC CHUNK POST CHUNK CREATOR: ModelResponseStream(id='chatcmpl-234dc04a-6673-41c7-9697-d5a2381a40a0', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=38, prompt_tokens=842, total_tokens=880, completion_tokens_details=None, prompt_tokens_details=None))
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: proxy_server.py:3282 - async_data_generator: received streaming chunk - ModelResponseStream(id='chatcmpl-234dc04a-6673-41c7-9697-d5a2381a40a0', created=1770562220, model='deepseek-coder:6.7b', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:349 - Logging Details LiteLLM-Async Success Call, cache_hit=False
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:349 - Async success callbacks: Got a complete streaming response
[92m14:50:24 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:50:24 - LiteLLM:DEBUG[0m: litellm_logging.py:1603 - Logging Details LiteLLM-Success Call: Cache_hit=False
[92m14:50:24 - LiteLLM:DEBUG[0m: litellm_logging.py:1632 - Logging Details LiteLLM-Success Call streaming complete
[92m14:50:24 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:50:24 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:50:24 - LiteLLM:DEBUG[0m: litellm_logging.py:2172 - Model=deepseek-coder:6.7b; cost=0.0
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:50:24 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:151 - in RouterBudgetLimiting.async_log_success_event
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:167 - Not running _PROXY_VirtualKeyModelMaxBudgetLimiter.async_log_success_event because user_api_key_model_max_budget is None or empty. `user_api_key_model_max_budget`={}
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.sync_deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - INSIDE parallel request limiter ASYNC SUCCESS LOGGING
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 880, 'current_rpm': 1}, precise_minute: 2026-02-08-14-50
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 880, 'current_rpm': 0}, precise_minute: 2026-02-08-14-50
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: common_request_processing.py:388 - Request received by LiteLLM:
{
    "messages": [
        {
            "role": "system",
            "content": "You are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Is written in English.\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., \"add feature\" not \"added feature\" or \"adding feature\").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n"
        },
        {
            "role": "user",
            "content": "\nUSER: Create hello.py with a function greet() that prints 'Hello from Lodestar'\n\nASSISTANT: Sure, here is the content for hello.py:\n\nhello.py\n```\ndef greet():\n    print('Hello from Lodestar')\n```\n\n# Diffs:\ndiff --git a/hello.py b/hello.py\nnew file mode 100644\nindex 0000000..e69de29diff --git a/hello.py b/hello.py\nindex e69de29..c7770f3 100644\n--- a/hello.py\n+++ b/hello.py\n@@ -0,0 +1,2 @@\n+def greet():\n+    print('Hello from Lodestar')"
        }
    ],
    "model": "gpt-3.5-turbo",
    "temperature": 0
}
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:664 - Request Headers: Headers({'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '1357'})
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:670 - receiving data: {'messages': [{'role': 'system', 'content': 'You are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Is written in English.\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., "add feature" not "added feature" or "adding feature").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n'}, {'role': 'user', 'content': "\nUSER: Create hello.py with a function greet() that prints 'Hello from Lodestar'\n\nASSISTANT: Sure, here is the content for hello.py:\n\nhello.py\n```\ndef greet():\n    print('Hello from Lodestar')\n```\n\n# Diffs:\ndiff --git a/hello.py b/hello.py\nnew file mode 100644\nindex 0000000..e69de29diff --git a/hello.py b/hello.py\nindex e69de29..c7770f3 100644\n--- a/hello.py\n+++ b/hello.py\n@@ -0,0 +1,2 @@\n+def greet():\n+    print('Hello from Lodestar')"}], 'model': 'gpt-3.5-turbo', 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '1357'}, 'body': {'messages': [{'role': 'system', 'content': 'You are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Is written in English.\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., "add feature" not "added feature" or "adding feature").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n'}, {'role': 'user', 'content': "\nUSER: Create hello.py with a function greet() that prints 'Hello from Lodestar'\n\nASSISTANT: Sure, here is the content for hello.py:\n\nhello.py\n```\ndef greet():\n    print('Hello from Lodestar')\n```\n\n# Diffs:\ndiff --git a/hello.py b/hello.py\nnew file mode 100644\nindex 0000000..e69de29diff --git a/hello.py b/hello.py\nindex e69de29..c7770f3 100644\n--- a/hello.py\n+++ b/hello.py\n@@ -0,0 +1,2 @@\n+def greet():\n+    print('Hello from Lodestar')"}], 'model': 'gpt-3.5-turbo', 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '1357'}}}
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: litellm_pre_call_utils.py:839 - [PROXY] returned data from litellm_pre_call_utils: {'messages': [{'role': 'system', 'content': 'You are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Is written in English.\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., "add feature" not "added feature" or "adding feature").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n'}, {'role': 'user', 'content': "\nUSER: Create hello.py with a function greet() that prints 'Hello from Lodestar'\n\nASSISTANT: Sure, here is the content for hello.py:\n\nhello.py\n```\ndef greet():\n    print('Hello from Lodestar')\n```\n\n# Diffs:\ndiff --git a/hello.py b/hello.py\nnew file mode 100644\nindex 0000000..e69de29diff --git a/hello.py b/hello.py\nindex e69de29..c7770f3 100644\n--- a/hello.py\n+++ b/hello.py\n@@ -0,0 +1,2 @@\n+def greet():\n+    print('Hello from Lodestar')"}], 'model': 'gpt-3.5-turbo', 'temperature': 0, 'proxy_server_request': {'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '1357'}, 'body': {'messages': [{'role': 'system', 'content': 'You are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Is written in English.\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., "add feature" not "added feature" or "adding feature").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n'}, {'role': 'user', 'content': "\nUSER: Create hello.py with a function greet() that prints 'Hello from Lodestar'\n\nASSISTANT: Sure, here is the content for hello.py:\n\nhello.py\n```\ndef greet():\n    print('Hello from Lodestar')\n```\n\n# Diffs:\ndiff --git a/hello.py b/hello.py\nnew file mode 100644\nindex 0000000..e69de29diff --git a/hello.py b/hello.py\nindex e69de29..c7770f3 100644\n--- a/hello.py\n+++ b/hello.py\n@@ -0,0 +1,2 @@\n+def greet():\n+    print('Hello from Lodestar')"}], 'model': 'gpt-3.5-turbo', 'temperature': 0}}, 'secret_fields': {'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '1357'}}, 'metadata': {'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '1357'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': ''}}
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:349 - Initialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510>, <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0>, <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0>]
[92m14:50:24 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {}
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: utils.py:1025 - Inside Proxy Logging Pre-call hook!
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: max_budget_limiter.py:23 - Inside Max Budget Limiter Pre-Call Hook
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - Inside Max Parallel Request Pre-Call Hook
[92m14:50:24 - LiteLLM Proxy:INFO[0m: parallel_request_limiter.py:68 - Current Usage of key in this minute: {'current_requests': 0, 'current_tpm': 880, 'current_rpm': 1}
[92m14:50:24 - LiteLLM Proxy:DEBUG[0m: cache_control_check.py:27 - Inside Cache Control Check Pre-Call Hook
[92m14:50:24 - LiteLLM Router:DEBUG[0m: router.py:3959 - Inside async function with retries.
[92m14:50:24 - LiteLLM Router:DEBUG[0m: router.py:3979 - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x7f4a04698690>>, num_retries - 3
[92m14:50:24 - LiteLLM Router:DEBUG[0m: router.py:6621 - initial list of deployments: [{'model_name': 'gpt-3.5-turbo', 'litellm_params': {'api_base': 'http://192.168.120.211:11434', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'ollama/deepseek-coder:6.7b'}, 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}}]
[92m14:50:24 - LiteLLM Router:DEBUG[0m: cooldown_handlers.py:342 - retrieve cooldown models: []
[92m14:50:24 - LiteLLM Router:DEBUG[0m: router.py:6688 - async cooldown deployments: []
[92m14:50:24 - LiteLLM Router:DEBUG[0m: router.py:6691 - cooldown_deployments: []
[92m14:50:24 - LiteLLM Router:DEBUG[0m: router.py:7055 - cooldown deployments: []
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:349 - [92mRequest to litellm:[0m
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:349 - [92mlitellm.acompletion(api_base='http://192.168.120.211:11434', use_in_pass_through=False, use_litellm_proxy=False, merge_reasoning_content_in_choices=False, model='ollama/deepseek-coder:6.7b', messages=[{'role': 'system', 'content': 'You are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Is written in English.\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., "add feature" not "added feature" or "adding feature").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n'}, {'role': 'user', 'content': "\nUSER: Create hello.py with a function greet() that prints 'Hello from Lodestar'\n\nASSISTANT: Sure, here is the content for hello.py:\n\nhello.py\n```\ndef greet():\n    print('Hello from Lodestar')\n```\n\n# Diffs:\ndiff --git a/hello.py b/hello.py\nnew file mode 100644\nindex 0000000..e69de29diff --git a/hello.py b/hello.py\nindex e69de29..c7770f3 100644\n--- a/hello.py\n+++ b/hello.py\n@@ -0,0 +1,2 @@\n+def greet():\n+    print('Hello from Lodestar')"}], caching=False, client=None, temperature=0, proxy_server_request={'url': 'http://localhost:4000/v1/chat/completions', 'method': 'POST', 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '1357'}, 'body': {'messages': [{'role': 'system', 'content': 'You are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Is written in English.\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., "add feature" not "added feature" or "adding feature").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n'}, {'role': 'user', 'content': "\nUSER: Create hello.py with a function greet() that prints 'Hello from Lodestar'\n\nASSISTANT: Sure, here is the content for hello.py:\n\nhello.py\n```\ndef greet():\n    print('Hello from Lodestar')\n```\n\n# Diffs:\ndiff --git a/hello.py b/hello.py\nnew file mode 100644\nindex 0000000..e69de29diff --git a/hello.py b/hello.py\nindex e69de29..c7770f3 100644\n--- a/hello.py\n+++ b/hello.py\n@@ -0,0 +1,2 @@\n+def greet():\n+    print('Hello from Lodestar')"}], 'model': 'gpt-3.5-turbo', 'temperature': 0}}, secret_fields={'raw_headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'authorization': 'Bearer sk-1234', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '1357'}}, metadata={'requester_metadata': {}, 'user_api_key_hash': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_user_id': None, 'user_api_key_org_id': None, 'user_api_key_team_alias': None, 'user_api_key_end_user_id': None, 'user_api_key_user_email': None, 'user_api_key_request_route': '/v1/chat/completions', 'user_api_key': '88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b', 'user_api_end_user_max_budget': None, 'litellm_api_version': '1.75.0', 'global_max_parallel_requests': None, 'user_api_key_team_max_budget': None, 'user_api_key_team_spend': None, 'user_api_key_spend': 0.0, 'user_api_key_max_budget': None, 'user_api_key_model_max_budget': {}, 'user_api_key_metadata': {}, 'headers': {'host': 'localhost:4000', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.99.1', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.99.1', 'x-stainless-os': 'Linux', 'x-stainless-arch': 'x64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.2', 'x-stainless-async': 'false', 'x-stainless-raw-response': 'true', 'x-stainless-retry-count': '0', 'x-stainless-read-timeout': '600.0', 'content-length': '1357'}, 'endpoint': 'http://localhost:4000/v1/chat/completions', 'litellm_parent_otel_span': None, 'requester_ip_address': '', 'model_group': 'gpt-3.5-turbo', 'model_group_alias': None, 'model_group_size': 1, 'deployment': 'ollama/deepseek-coder:6.7b', 'model_info': {'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, 'api_base': 'http://192.168.120.211:11434', 'deployment_model_name': 'gpt-3.5-turbo', 'caching_groups': None}, litellm_call_id='a43070c8-acd4-4c36-9d77-ee79a3908dd6', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x7f4a009f8450>, stream=False, litellm_trace_id='c87b327f-37ca-4921-87e0-61b8b14d42e6', model_info={'id': '8df02045eed373e2a6ce271f0dbdf0315cb0638e7c353ee7ebcb9d977ae6bc1b', 'db_model': False}, timeout=600, max_retries=0)[0m
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:349 - 

[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:349 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
[92m14:50:24 - LiteLLM:DEBUG[0m: caching_handler.py:213 - CACHE RESULT: None
[92m14:50:24 - LiteLLM:INFO[0m: utils.py:3260 - 
LiteLLM completion() model= deepseek-coder:6.7b; provider = ollama
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:3263 - 
LiteLLM: Params passed to completion() {'model': 'deepseek-coder:6.7b', 'functions': None, 'function_call': None, 'temperature': 0, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Is written in English.\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., "add feature" not "added feature" or "adding feature").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n'}, {'role': 'user', 'content': "\nUSER: Create hello.py with a function greet() that prints 'Hello from Lodestar'\n\nASSISTANT: Sure, here is the content for hello.py:\n\nhello.py\n```\ndef greet():\n    print('Hello from Lodestar')\n```\n\n# Diffs:\ndiff --git a/hello.py b/hello.py\nnew file mode 100644\nindex 0000000..e69de29diff --git a/hello.py b/hello.py\nindex e69de29..c7770f3 100644\n--- a/hello.py\n+++ b/hello.py\n@@ -0,0 +1,2 @@\n+def greet():\n+    print('Hello from Lodestar')"}], 'thinking': None, 'web_search_options': None}
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:3266 - 
LiteLLM: Non-Default params passed to completion() {'temperature': 0, 'stream': False, 'max_retries': 0}
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:349 - Final returned optional params: {'temperature': 0, 'stream': False}
[92m14:50:24 - LiteLLM:DEBUG[0m: litellm_logging.py:475 - self.optional_params: {'temperature': 0, 'stream': False, 'max_retries': 0}
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4720 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: utils.py:4732 - model=deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:24 - LiteLLM:DEBUG[0m: litellm_logging.py:930 - [92m

POST Request Sent from LiteLLM:
curl -X POST \
http://192.168.120.211:11434/api/generate \
-d '{'model': 'deepseek-coder:6.7b', 'prompt': '### System:\nYou are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Is written in English.\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., "add feature" not "added feature" or "adding feature").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n\n\n### User:\n\nUSER: Create hello.py with a function greet() that prints \'Hello from Lodestar\'\n\nASSISTANT: Sure, here is the content for hello.py:\n\nhello.py\n```\ndef greet():\n    print(\'Hello from Lodestar\')\n```\n\n# Diffs:\ndiff --git a/hello.py b/hello.py\nnew file mode 100644\nindex 0000000..e69de29diff --git a/hello.py b/hello.py\nindex e69de29..c7770f3 100644\n--- a/hello.py\n+++ b/hello.py\n@@ -0,0 +1,2 @@\n+def greet():\n+    print(\'Hello from Lodestar\')\n\n', 'options': {'temperature': 0}, 'stream': False, 'images': []}'
[0m

[92m14:50:33 - LiteLLM:DEBUG[0m: litellm_logging.py:2833 - Filtered callbacks: []
[92m14:50:33 - LiteLLM:DEBUG[0m: cost_calculator.py:675 - selected model name for cost calculation: ollama/deepseek-coder:6.7b
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:33 - LiteLLM:DEBUG[0m: cost_calculator.py:394 - Returned custom cost for model=ollama/deepseek-coder:6.7b - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
[92m14:50:33 - LiteLLM:DEBUG[0m: litellm_logging.py:1232 - response_cost: 0.0
[92m14:50:33 - LiteLLM Router:INFO[0m: router.py:1331 - litellm.acompletion(model=ollama/deepseek-coder:6.7b)[32m 200 OK[0m
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'ollama/deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:50:33 - LiteLLM Router:DEBUG[0m: router.py:3892 - Async Response: ModelResponse(id='chatcmpl-32687ceb-19b1-415e-93b5-b7aa6e8757c2', created=1770562233, model='ollama/deepseek-coder:6.7b', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='feat: Add greet() function to hello.py\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=14, prompt_tokens=482, total_tokens=496, completion_tokens_details=None, prompt_tokens_details=None))
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:349 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x7f4a009f8450>>
[92m14:50:33 - LiteLLM:DEBUG[0m: litellm_logging.py:2833 - Filtered callbacks: []
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:349 - Logging Details LiteLLM-Async Success Call, cache_hit=None
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:349 - Async success callbacks: Got a complete streaming response
[92m14:50:33 - LiteLLM:DEBUG[0m: litellm_logging.py:2172 - Model=deepseek-coder:6.7b; cost=0.0
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4622 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-coder:6.7b', 'combined_model_name': 'ollama/deepseek-coder:6.7b', 'stripped_model_name': 'deepseek-coder:6.7b', 'combined_stripped_model_name': 'ollama/deepseek-coder:6.7b', 'custom_llm_provider': 'ollama'}
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4720 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no input_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4732 - model=ollama/deepseek-coder:6.7b, custom_llm_provider=ollama has no output_cost_per_token in model_cost_map. Defaulting to 0.
[92m14:50:33 - LiteLLM:DEBUG[0m: utils.py:4927 - model_info: {'key': 'ollama/deepseek-coder:6.7b', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'ollama', 'mode': None, 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f4a04698690>> is disabled via headers. Disable callbacks from headers: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f4a0479f510> is disabled via headers. Disable callbacks from headers: None
[92m14:50:33 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:151 - in RouterBudgetLimiting.async_log_success_event
[92m14:50:33 - LiteLLM Proxy:DEBUG[0m: model_max_budget_limiter.py:167 - Not running _PROXY_VirtualKeyModelMaxBudgetLimiter.async_log_success_event because user_api_key_model_max_budget is None or empty. `user_api_key_model_max_budget`={}
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.router_utils.forward_clientside_headers_by_model_group.ForwardClientSideHeadersByModelGroup object at 0x7f4a0493aed0> is disabled via headers. Disable callbacks from headers: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f4a0502ad50> is disabled via headers. Disable callbacks from headers: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f4a0417d450> is disabled via headers. Disable callbacks from headers: None
[92m14:50:33 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - INSIDE parallel request limiter ASYNC SUCCESS LOGGING
[92m14:50:33 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 1376, 'current_rpm': 2}, precise_minute: 2026-02-08-14-50
[92m14:50:33 - LiteLLM Proxy:DEBUG[0m: parallel_request_limiter.py:48 - updated_value in success call: {'current_requests': 0, 'current_tpm': 1376, 'current_rpm': 0}, precise_minute: 2026-02-08-14-50
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f4a0405b8d0> is disabled via headers. Disable callbacks from headers: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:37 - Dynamically disabled callbacks from x-litellm-disable-callbacks: None
[92m14:50:33 - LiteLLM:DEBUG[0m: callback_controls.py:38 - Checking if <litellm._service_logger.ServiceLogging object at 0x7f4a04a49fd0> is disabled via headers. Disable callbacks from headers: None
INFO:     127.0.0.1:35696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[92m14:51:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m14:54:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m14:56:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m14:59:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:01:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:04:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:06:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:09:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:11:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:14:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:16:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:19:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:21:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:24:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:26:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:29:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:31:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:34:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:36:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:39:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:41:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:44:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:46:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:49:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:51:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:54:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:56:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m15:59:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m16:01:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m16:04:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m16:06:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m16:09:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m16:11:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m16:14:06 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
[92m16:16:36 - LiteLLM Proxy:DEBUG[0m: hanging_request_check.py:148 - Checking for hanging requests....
